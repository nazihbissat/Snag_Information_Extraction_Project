{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from postings_ner import *\n",
    "import spacy\n",
    "import pprint as pp\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pkl_file = open('postings-6-29.pkl', 'rb')\n",
    "results = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_entity_types = {'ORG': 'Organization', 'PRODUCT': 'Product', 'CARDINAL': 'Cardinal', 'PERSON': 'Person',\n",
    "                     'TIME': 'Time', 'DATE': 'Date', 'PERCENT': 'Percent', 'GPE': 'Location_Geo-Political_Area',\n",
    "                     'WORK_OF_ART': 'Job_Title_or_Degree', 'LANGUAGE': 'Language', 'QUANTITY': 'Quantity',\n",
    "                     'EVENT': 'Event', 'MONEY': 'Money', 'NORP': 'Nationality_or_Religion_or_Political_group',\n",
    "                     'ORDINAL': 'Ordinal', 'FAC': 'Facility', 'LOC': 'Location', 'LAW': 'Law'}\n",
    "\n",
    "stfd_entity_types = {'ORGANIZATION': 'Organization', 'TITLE': 'Title', 'LOCATION': 'Location', 'EMAIL': 'Email',\n",
    "               'URL': 'URL', 'CITY': 'Location_City', 'STATE_OR_PROVINCE': 'Location_State_or_Province',\n",
    "               'COUNTRY': 'Location_Country', 'NATIONALITY': 'Nationality', 'RELIGION': 'Religion', 'TITLE': 'Title',\n",
    "               'IDEOLOGY': 'Ideology', 'CRIMINAL_CHARGE': 'Criminal_Charge', 'CAUSE_OF_DEATH': 'Cause_of_Death',\n",
    "               'PERSON': 'Person', 'MONEY': 'Money', 'TIME': 'Time', 'ORDINAL': 'Ordinal', 'SET': 'Set',\n",
    "               'DURATION': 'Duration', 'NUMBER': 'Number', 'PERCENT': 'Percent', 'MISC': 'Miscellaneous',\n",
    "                'DATE': 'Date'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'Hello my name is Nazih Bissat and I am a data scientist intern at Snag.'\n",
    "\n",
    "# doc = nlp(test)\n",
    "# print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description.\n",
      "\n",
      "HUB Order Puller.\n",
      "\n",
      "At Advance Auto Parts, a HUB Order Puller is primarily responsible for pulling HUB customer orders.\n",
      "\n",
      "ESSENTIAL DUTIES AND RESPONSIBILITIES include the following. Other duties may be assigned.\n",
      "\n",
      " Pull customer orders and prepare for delivery.\n",
      " Assist with the receiving and unloading of replenishment stock.\n",
      " Store replenishment stock in proper location.\n",
      " Accurately and legibly complete all paperwork related to the task of an order puller.\n",
      " Properly maintain equipment and report any problems on a timely basis; keep work area clean and neat.\n",
      " Operate at a professional level of efficiency in order not to breech company security.\n",
      " Work with store management team to complete MAXI changes in parts department.\n",
      " Comply with all federal, state and local laws.\n",
      " Comply with all company policies and procedures.\n",
      " Complete all required training materials, attend all scheduled company meetings.\n",
      "\n",
      "Job Requirements.\n",
      "\n",
      "HUB Order Puller.\n",
      "\n",
      "QUALIFICATIONS.\n",
      "\n",
      "To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\n",
      "\n",
      " Able to use a hand truck and pallet jack.\n",
      " Able to follow detailed instructions.\n",
      " Able to work efficiently with others both within and outside the store.\n",
      " Able to perform successfully with minimum supervision.\n",
      " Maintain a good personal appearance.\n",
      " Scheduling factors necessitate that the individual should be able to come to work on short notice, and/or work flexible hours including nights and weekends.\n",
      "\n",
      "EDUCATION and/or EXPERIENCE.\n",
      "\n",
      " High school diploma or GED equivalent; or one to three months related experience and/or training; or equivalent combination or education and experience.\n",
      "\n",
      "LANGUAGE SKILLS.\n",
      "\n",
      " Ability to read and comprehend simple instruction, short correspondence, and memos.\n",
      " Ability to write simple correspondence.\n",
      " Ability to effectively present information in one on one and small group situations to customer, clients and other Team Members of the organization.\n",
      "\n",
      "MATHEMATICAL SKILLS.\n",
      "\n",
      " Ability to add, subtract, multiply, and divide in all units of measure, using whole numbers, common fractions, and decimals.\n",
      " Ability to compute rates, ratios, percentages and interprets bar graphs.\n",
      "\n",
      "REASONING ABILITY.\n",
      "\n",
      " Ability to define problems collects and evaluates information, establish facts and draw valid conclusions.\n",
      " Ability to interpret Advance policies and\n"
     ]
    }
   ],
   "source": [
    "# JUST TO SEE A POSTING\n",
    "def insertPeriod(position, mystring):\n",
    "    longi = len(mystring)\n",
    "    mystring   =  mystring[:position] + '.' + mystring[position:] \n",
    "    return mystring \n",
    "\n",
    "r = results[0]\n",
    "posting_text = r['JD_SCRUBBED'].strip()\n",
    "posting_text = re.sub(r'(\\n-)', '\\n', posting_text)\n",
    "posting_text = re.sub(r'(\\n  -)', '\\n', posting_text)\n",
    "posting_text = re.sub(r'(\\n  -)', '\\n', posting_text)\n",
    "posting_text = re.sub(r'(\\n  )', '\\n', posting_text)\n",
    "\n",
    "indices = [x.start() for x in re.finditer(r'\\n', posting_text)]\n",
    "\n",
    "counter = 0\n",
    "for i in indices:\n",
    "    if posting_text[i+counter-1] != '.':\n",
    "        if posting_text[i+counter-1] != '\\n':\n",
    "            posting_text = insertPeriod(i+counter, posting_text)\n",
    "            counter += 1\n",
    "print(posting_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StanfordNLP NER tagger training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StanfordNLP dependency parsing\n",
    "startup_corenlp_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "depparse = parse_posting(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentences': [{'basicDependencies': [{'dep': 'ROOT',\n",
      "                                       'dependent': 6,\n",
      "                                       'dependentGloss': 'Bissat',\n",
      "                                       'governor': 0,\n",
      "                                       'governorGloss': 'ROOT'},\n",
      "                                      {'dep': 'advmod',\n",
      "                                       'dependent': 1,\n",
      "                                       'dependentGloss': 'Hello',\n",
      "                                       'governor': 6,\n",
      "                                       'governorGloss': 'Bissat'},\n",
      "                                      {'dep': 'nmod:poss',\n",
      "                                       'dependent': 2,\n",
      "                                       'dependentGloss': 'my',\n",
      "                                       'governor': 3,\n",
      "                                       'governorGloss': 'name'},\n",
      "                                      {'dep': 'nsubj',\n",
      "                                       'dependent': 3,\n",
      "                                       'dependentGloss': 'name',\n",
      "                                       'governor': 6,\n",
      "                                       'governorGloss': 'Bissat'},\n",
      "                                      {'dep': 'cop',\n",
      "                                       'dependent': 4,\n",
      "                                       'dependentGloss': 'is',\n",
      "                                       'governor': 6,\n",
      "                                       'governorGloss': 'Bissat'},\n",
      "                                      {'dep': 'compound',\n",
      "                                       'dependent': 5,\n",
      "                                       'dependentGloss': 'Nazih',\n",
      "                                       'governor': 6,\n",
      "                                       'governorGloss': 'Bissat'},\n",
      "                                      {'dep': 'cc',\n",
      "                                       'dependent': 7,\n",
      "                                       'dependentGloss': 'and',\n",
      "                                       'governor': 6,\n",
      "                                       'governorGloss': 'Bissat'},\n",
      "                                      {'dep': 'nsubj',\n",
      "                                       'dependent': 8,\n",
      "                                       'dependentGloss': 'I',\n",
      "                                       'governor': 13,\n",
      "                                       'governorGloss': 'intern'},\n",
      "                                      {'dep': 'cop',\n",
      "                                       'dependent': 9,\n",
      "                                       'dependentGloss': 'am',\n",
      "                                       'governor': 13,\n",
      "                                       'governorGloss': 'intern'},\n",
      "                                      {'dep': 'det',\n",
      "                                       'dependent': 10,\n",
      "                                       'dependentGloss': 'a',\n",
      "                                       'governor': 13,\n",
      "                                       'governorGloss': 'intern'},\n",
      "                                      {'dep': 'compound',\n",
      "                                       'dependent': 11,\n",
      "                                       'dependentGloss': 'data',\n",
      "                                       'governor': 13,\n",
      "                                       'governorGloss': 'intern'},\n",
      "                                      {'dep': 'compound',\n",
      "                                       'dependent': 12,\n",
      "                                       'dependentGloss': 'scientist',\n",
      "                                       'governor': 13,\n",
      "                                       'governorGloss': 'intern'},\n",
      "                                      {'dep': 'conj',\n",
      "                                       'dependent': 13,\n",
      "                                       'dependentGloss': 'intern',\n",
      "                                       'governor': 6,\n",
      "                                       'governorGloss': 'Bissat'},\n",
      "                                      {'dep': 'case',\n",
      "                                       'dependent': 14,\n",
      "                                       'dependentGloss': 'at',\n",
      "                                       'governor': 15,\n",
      "                                       'governorGloss': 'Snag'},\n",
      "                                      {'dep': 'nmod',\n",
      "                                       'dependent': 15,\n",
      "                                       'dependentGloss': 'Snag',\n",
      "                                       'governor': 13,\n",
      "                                       'governorGloss': 'intern'},\n",
      "                                      {'dep': 'punct',\n",
      "                                       'dependent': 16,\n",
      "                                       'dependentGloss': '.',\n",
      "                                       'governor': 6,\n",
      "                                       'governorGloss': 'Bissat'}],\n",
      "                'enhancedDependencies': [{'dep': 'ROOT',\n",
      "                                          'dependent': 6,\n",
      "                                          'dependentGloss': 'Bissat',\n",
      "                                          'governor': 0,\n",
      "                                          'governorGloss': 'ROOT'},\n",
      "                                         {'dep': 'advmod',\n",
      "                                          'dependent': 1,\n",
      "                                          'dependentGloss': 'Hello',\n",
      "                                          'governor': 6,\n",
      "                                          'governorGloss': 'Bissat'},\n",
      "                                         {'dep': 'nmod:poss',\n",
      "                                          'dependent': 2,\n",
      "                                          'dependentGloss': 'my',\n",
      "                                          'governor': 3,\n",
      "                                          'governorGloss': 'name'},\n",
      "                                         {'dep': 'nsubj',\n",
      "                                          'dependent': 3,\n",
      "                                          'dependentGloss': 'name',\n",
      "                                          'governor': 6,\n",
      "                                          'governorGloss': 'Bissat'},\n",
      "                                         {'dep': 'cop',\n",
      "                                          'dependent': 4,\n",
      "                                          'dependentGloss': 'is',\n",
      "                                          'governor': 6,\n",
      "                                          'governorGloss': 'Bissat'},\n",
      "                                         {'dep': 'compound',\n",
      "                                          'dependent': 5,\n",
      "                                          'dependentGloss': 'Nazih',\n",
      "                                          'governor': 6,\n",
      "                                          'governorGloss': 'Bissat'},\n",
      "                                         {'dep': 'cc',\n",
      "                                          'dependent': 7,\n",
      "                                          'dependentGloss': 'and',\n",
      "                                          'governor': 6,\n",
      "                                          'governorGloss': 'Bissat'},\n",
      "                                         {'dep': 'nsubj',\n",
      "                                          'dependent': 8,\n",
      "                                          'dependentGloss': 'I',\n",
      "                                          'governor': 13,\n",
      "                                          'governorGloss': 'intern'},\n",
      "                                         {'dep': 'cop',\n",
      "                                          'dependent': 9,\n",
      "                                          'dependentGloss': 'am',\n",
      "                                          'governor': 13,\n",
      "                                          'governorGloss': 'intern'},\n",
      "                                         {'dep': 'det',\n",
      "                                          'dependent': 10,\n",
      "                                          'dependentGloss': 'a',\n",
      "                                          'governor': 13,\n",
      "                                          'governorGloss': 'intern'},\n",
      "                                         {'dep': 'compound',\n",
      "                                          'dependent': 11,\n",
      "                                          'dependentGloss': 'data',\n",
      "                                          'governor': 13,\n",
      "                                          'governorGloss': 'intern'},\n",
      "                                         {'dep': 'compound',\n",
      "                                          'dependent': 12,\n",
      "                                          'dependentGloss': 'scientist',\n",
      "                                          'governor': 13,\n",
      "                                          'governorGloss': 'intern'},\n",
      "                                         {'dep': 'conj:and',\n",
      "                                          'dependent': 13,\n",
      "                                          'dependentGloss': 'intern',\n",
      "                                          'governor': 6,\n",
      "                                          'governorGloss': 'Bissat'},\n",
      "                                         {'dep': 'case',\n",
      "                                          'dependent': 14,\n",
      "                                          'dependentGloss': 'at',\n",
      "                                          'governor': 15,\n",
      "                                          'governorGloss': 'Snag'},\n",
      "                                         {'dep': 'nmod:at',\n",
      "                                          'dependent': 15,\n",
      "                                          'dependentGloss': 'Snag',\n",
      "                                          'governor': 13,\n",
      "                                          'governorGloss': 'intern'},\n",
      "                                         {'dep': 'punct',\n",
      "                                          'dependent': 16,\n",
      "                                          'dependentGloss': '.',\n",
      "                                          'governor': 6,\n",
      "                                          'governorGloss': 'Bissat'}],\n",
      "                'enhancedPlusPlusDependencies': [{'dep': 'ROOT',\n",
      "                                                  'dependent': 6,\n",
      "                                                  'dependentGloss': 'Bissat',\n",
      "                                                  'governor': 0,\n",
      "                                                  'governorGloss': 'ROOT'},\n",
      "                                                 {'dep': 'advmod',\n",
      "                                                  'dependent': 1,\n",
      "                                                  'dependentGloss': 'Hello',\n",
      "                                                  'governor': 6,\n",
      "                                                  'governorGloss': 'Bissat'},\n",
      "                                                 {'dep': 'nmod:poss',\n",
      "                                                  'dependent': 2,\n",
      "                                                  'dependentGloss': 'my',\n",
      "                                                  'governor': 3,\n",
      "                                                  'governorGloss': 'name'},\n",
      "                                                 {'dep': 'nsubj',\n",
      "                                                  'dependent': 3,\n",
      "                                                  'dependentGloss': 'name',\n",
      "                                                  'governor': 6,\n",
      "                                                  'governorGloss': 'Bissat'},\n",
      "                                                 {'dep': 'cop',\n",
      "                                                  'dependent': 4,\n",
      "                                                  'dependentGloss': 'is',\n",
      "                                                  'governor': 6,\n",
      "                                                  'governorGloss': 'Bissat'},\n",
      "                                                 {'dep': 'compound',\n",
      "                                                  'dependent': 5,\n",
      "                                                  'dependentGloss': 'Nazih',\n",
      "                                                  'governor': 6,\n",
      "                                                  'governorGloss': 'Bissat'},\n",
      "                                                 {'dep': 'cc',\n",
      "                                                  'dependent': 7,\n",
      "                                                  'dependentGloss': 'and',\n",
      "                                                  'governor': 6,\n",
      "                                                  'governorGloss': 'Bissat'},\n",
      "                                                 {'dep': 'nsubj',\n",
      "                                                  'dependent': 8,\n",
      "                                                  'dependentGloss': 'I',\n",
      "                                                  'governor': 13,\n",
      "                                                  'governorGloss': 'intern'},\n",
      "                                                 {'dep': 'cop',\n",
      "                                                  'dependent': 9,\n",
      "                                                  'dependentGloss': 'am',\n",
      "                                                  'governor': 13,\n",
      "                                                  'governorGloss': 'intern'},\n",
      "                                                 {'dep': 'det',\n",
      "                                                  'dependent': 10,\n",
      "                                                  'dependentGloss': 'a',\n",
      "                                                  'governor': 13,\n",
      "                                                  'governorGloss': 'intern'},\n",
      "                                                 {'dep': 'compound',\n",
      "                                                  'dependent': 11,\n",
      "                                                  'dependentGloss': 'data',\n",
      "                                                  'governor': 13,\n",
      "                                                  'governorGloss': 'intern'},\n",
      "                                                 {'dep': 'compound',\n",
      "                                                  'dependent': 12,\n",
      "                                                  'dependentGloss': 'scientist',\n",
      "                                                  'governor': 13,\n",
      "                                                  'governorGloss': 'intern'},\n",
      "                                                 {'dep': 'conj:and',\n",
      "                                                  'dependent': 13,\n",
      "                                                  'dependentGloss': 'intern',\n",
      "                                                  'governor': 6,\n",
      "                                                  'governorGloss': 'Bissat'},\n",
      "                                                 {'dep': 'case',\n",
      "                                                  'dependent': 14,\n",
      "                                                  'dependentGloss': 'at',\n",
      "                                                  'governor': 15,\n",
      "                                                  'governorGloss': 'Snag'},\n",
      "                                                 {'dep': 'nmod:at',\n",
      "                                                  'dependent': 15,\n",
      "                                                  'dependentGloss': 'Snag',\n",
      "                                                  'governor': 13,\n",
      "                                                  'governorGloss': 'intern'},\n",
      "                                                 {'dep': 'punct',\n",
      "                                                  'dependent': 16,\n",
      "                                                  'dependentGloss': '.',\n",
      "                                                  'governor': 6,\n",
      "                                                  'governorGloss': 'Bissat'}],\n",
      "                'index': 0,\n",
      "                'tokens': [{'after': ' ',\n",
      "                            'before': '',\n",
      "                            'characterOffsetBegin': 0,\n",
      "                            'characterOffsetEnd': 5,\n",
      "                            'index': 1,\n",
      "                            'originalText': 'Hello',\n",
      "                            'pos': 'UH',\n",
      "                            'word': 'Hello'},\n",
      "                           {'after': ' ',\n",
      "                            'before': ' ',\n",
      "                            'characterOffsetBegin': 6,\n",
      "                            'characterOffsetEnd': 8,\n",
      "                            'index': 2,\n",
      "                            'originalText': 'my',\n",
      "                            'pos': 'PRP$',\n",
      "                            'word': 'my'},\n",
      "                           {'after': ' ',\n",
      "                            'before': ' ',\n",
      "                            'characterOffsetBegin': 9,\n",
      "                            'characterOffsetEnd': 13,\n",
      "                            'index': 3,\n",
      "                            'originalText': 'name',\n",
      "                            'pos': 'NN',\n",
      "                            'word': 'name'},\n",
      "                           {'after': ' ',\n",
      "                            'before': ' ',\n",
      "                            'characterOffsetBegin': 14,\n",
      "                            'characterOffsetEnd': 16,\n",
      "                            'index': 4,\n",
      "                            'originalText': 'is',\n",
      "                            'pos': 'VBZ',\n",
      "                            'word': 'is'},\n",
      "                           {'after': ' ',\n",
      "                            'before': ' ',\n",
      "                            'characterOffsetBegin': 17,\n",
      "                            'characterOffsetEnd': 22,\n",
      "                            'index': 5,\n",
      "                            'originalText': 'Nazih',\n",
      "                            'pos': 'NNP',\n",
      "                            'word': 'Nazih'},\n",
      "                           {'after': ' ',\n",
      "                            'before': ' ',\n",
      "                            'characterOffsetBegin': 23,\n",
      "                            'characterOffsetEnd': 29,\n",
      "                            'index': 6,\n",
      "                            'originalText': 'Bissat',\n",
      "                            'pos': 'NNP',\n",
      "                            'word': 'Bissat'},\n",
      "                           {'after': ' ',\n",
      "                            'before': ' ',\n",
      "                            'characterOffsetBegin': 30,\n",
      "                            'characterOffsetEnd': 33,\n",
      "                            'index': 7,\n",
      "                            'originalText': 'and',\n",
      "                            'pos': 'CC',\n",
      "                            'word': 'and'},\n",
      "                           {'after': ' ',\n",
      "                            'before': ' ',\n",
      "                            'characterOffsetBegin': 34,\n",
      "                            'characterOffsetEnd': 35,\n",
      "                            'index': 8,\n",
      "                            'originalText': 'I',\n",
      "                            'pos': 'PRP',\n",
      "                            'word': 'I'},\n",
      "                           {'after': ' ',\n",
      "                            'before': ' ',\n",
      "                            'characterOffsetBegin': 36,\n",
      "                            'characterOffsetEnd': 38,\n",
      "                            'index': 9,\n",
      "                            'originalText': 'am',\n",
      "                            'pos': 'VBP',\n",
      "                            'word': 'am'},\n",
      "                           {'after': ' ',\n",
      "                            'before': ' ',\n",
      "                            'characterOffsetBegin': 39,\n",
      "                            'characterOffsetEnd': 40,\n",
      "                            'index': 10,\n",
      "                            'originalText': 'a',\n",
      "                            'pos': 'DT',\n",
      "                            'word': 'a'},\n",
      "                           {'after': ' ',\n",
      "                            'before': ' ',\n",
      "                            'characterOffsetBegin': 41,\n",
      "                            'characterOffsetEnd': 45,\n",
      "                            'index': 11,\n",
      "                            'originalText': 'data',\n",
      "                            'pos': 'NN',\n",
      "                            'word': 'data'},\n",
      "                           {'after': ' ',\n",
      "                            'before': ' ',\n",
      "                            'characterOffsetBegin': 46,\n",
      "                            'characterOffsetEnd': 55,\n",
      "                            'index': 12,\n",
      "                            'originalText': 'scientist',\n",
      "                            'pos': 'NN',\n",
      "                            'word': 'scientist'},\n",
      "                           {'after': ' ',\n",
      "                            'before': ' ',\n",
      "                            'characterOffsetBegin': 56,\n",
      "                            'characterOffsetEnd': 62,\n",
      "                            'index': 13,\n",
      "                            'originalText': 'intern',\n",
      "                            'pos': 'NN',\n",
      "                            'word': 'intern'},\n",
      "                           {'after': ' ',\n",
      "                            'before': ' ',\n",
      "                            'characterOffsetBegin': 63,\n",
      "                            'characterOffsetEnd': 65,\n",
      "                            'index': 14,\n",
      "                            'originalText': 'at',\n",
      "                            'pos': 'IN',\n",
      "                            'word': 'at'},\n",
      "                           {'after': '',\n",
      "                            'before': ' ',\n",
      "                            'characterOffsetBegin': 66,\n",
      "                            'characterOffsetEnd': 70,\n",
      "                            'index': 15,\n",
      "                            'originalText': 'Snag',\n",
      "                            'pos': 'NN',\n",
      "                            'word': 'Snag'},\n",
      "                           {'after': '',\n",
      "                            'before': '',\n",
      "                            'characterOffsetBegin': 70,\n",
      "                            'characterOffsetEnd': 71,\n",
      "                            'index': 16,\n",
      "                            'originalText': '.',\n",
      "                            'pos': '.',\n",
      "                            'word': '.'}]}]}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(depparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING POSTING TEXT FILES AND RESPECTIVE ENTITY ANNOTATION FILES FOR BRAT TEXT ANNOTATION TOOL (StanfordNLP)\n",
    "\n",
    "def insertPeriod(position, mystring):\n",
    "    longi = len(mystring)\n",
    "    mystring   =  mystring[:position] + '.' + mystring[position:] \n",
    "    return mystring \n",
    "\n",
    "posting_index = 1\n",
    "for r in results:\n",
    "    posting_fname = 'posting' + str(posting_index)\n",
    "    posting_text = r['JD_SCRUBBED'].strip()\n",
    "    posting_text = re.sub(r'(\\n-)', '\\n', posting_text)\n",
    "    posting_text = re.sub(r'(\\n  -)', '\\n', posting_text)\n",
    "    posting_text = re.sub(r'(\\n  -)', '\\n', posting_text)\n",
    "    posting_text = re.sub(r'(\\n  )', '\\n', posting_text)\n",
    "    \n",
    "    indices = [x.start() for x in re.finditer(r'\\n', posting_text)]\n",
    "\n",
    "\n",
    "    counter = 0\n",
    "    for i in indices:\n",
    "        if posting_text[i+counter-1] != '.':\n",
    "            if posting_text[i+counter-1] != '\\n':\n",
    "                posting_text = insertPeriod(i+counter, posting_text)\n",
    "                counter += 1\n",
    "    \n",
    "    with open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/Parsing_Test/Test/' + posting_fname + '.txt', 'w') as text_file:\n",
    "        text_file.write(posting_text)\n",
    "        text_file.close()\n",
    "    \n",
    "    string_index = 1\n",
    "    dep_index = 1\n",
    "    parse_ann_file_text = ''\n",
    "    posting_details = parse_posting(posting_text)\n",
    "    word_count = 0\n",
    "    for s in posting_details['sentences']:\n",
    "        for t in s['tokens']:\n",
    "            parse_ann_file_text += 'T' + str(string_index) + '\\t' + t['pos'] + ' ' \\\n",
    "                                    + str(t['characterOffsetBegin']) + ' ' + str(t['characterOffsetEnd']) + '\\t' \\\n",
    "                                    + t['word'] + '\\n'\n",
    "            string_index += 1\n",
    "    \n",
    "        for d in s['basicDependencies']:\n",
    "            if d['dep'] != 'ROOT':\n",
    "                parse_ann_file_text += 'R' + str(dep_index) + '\\t' + d['dep'] + ' Arg1:T' \\\n",
    "                                        + str(d['governor'] + word_count) + ' Arg2:T' \\\n",
    "                                        + str(d['dependent'] + word_count) + '\\n'\n",
    "                dep_index += 1\n",
    "    \n",
    "        word_count = string_index - 1\n",
    "            \n",
    "    ann_file = open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/Parsing_Test/Test/' + posting_fname + '.ann', 'w')\n",
    "    ann_file.write(parse_ann_file_text)\n",
    "    ann_file.close()\n",
    "    \n",
    "    posting_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutdown_corenlp_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST FOR ABOVE CODE, SAME WITHOUT ITERATING OVER ALL POSTINGS, TRYING WITH INDIVIDUAL POSTING\n",
    "## NEXT: TRY TO DO PARSING SENTENCE BY SENTENCE\n",
    "\n",
    "def insertPeriod(position, mystring):\n",
    "    longi = len(mystring)\n",
    "    mystring   =  mystring[:position] + '.' + mystring[position:] \n",
    "    return mystring \n",
    "\n",
    "posting_text = results[99]['JD_SCRUBBED'].strip()\n",
    "posting_text = re.sub(r'(\\n-)', '\\n', posting_text)\n",
    "posting_text = re.sub(r'(\\n  -)', '\\n', posting_text)\n",
    "posting_text = re.sub(r'(\\n  -)', '\\n', posting_text)\n",
    "posting_text = re.sub(r'(\\n  )', '\\n', posting_text)\n",
    "    \n",
    "indices = [x.start() for x in re.finditer(r'\\n', posting_text)]\n",
    "\n",
    "counter = 0\n",
    "for i in indices:\n",
    "    if posting_text[i+counter-1] != '.':\n",
    "        if posting_text[i+counter-1] != '\\n':\n",
    "            posting_text = insertPeriod(i+counter, posting_text)\n",
    "            counter += 1\n",
    "    \n",
    "with open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/Parsing_Test/Test/test.txt', 'w') as text_file:\n",
    "        text_file.write(posting_text)\n",
    "        text_file.close()\n",
    "    \n",
    "string_index = 1\n",
    "dep_index = 1\n",
    "parse_ann_file_text = ''\n",
    "posting_details = parse_posting(posting_text)\n",
    "word_count = 0\n",
    "for s in posting_details['sentences']:\n",
    "    for t in s['tokens']:\n",
    "        parse_ann_file_text += 'T' + str(string_index) + '\\t' + t['pos'] + ' ' \\\n",
    "                                    + str(t['characterOffsetBegin']) + ' ' + str(t['characterOffsetEnd']) + '\\t' \\\n",
    "                                    + t['word'] + '\\n'\n",
    "        string_index += 1\n",
    "    \n",
    "    for d in s['basicDependencies']:\n",
    "        if d['dep'] != 'ROOT':\n",
    "            parse_ann_file_text += 'R' + str(dep_index) + '\\t' + d['dep'] + ' Arg1:T' \\\n",
    "                                        + str(d['governor'] + word_count) + ' Arg2:T' \\\n",
    "                                        + str(d['dependent'] + word_count) + '\\n'\n",
    "            dep_index += 1\n",
    "    \n",
    "    word_count = string_index - 1\n",
    "            \n",
    "            \n",
    "ann_file = open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/Parsing_Test/Test/test.ann', 'w')\n",
    "ann_file.write(parse_ann_file_text)\n",
    "ann_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job Description\\n\\nWhat is a Salesperson?\\n\\nEntry level sales position capable of supporting the DIY business and achieve our sales and service objectives. The role has good knowledge of store systems, basic automotive system knowledge and basic part knowledge. The role has the basic ability to source from stores, hubs, pdq, and external suppliers. The role has in-depth knowledge of the store inventory and maintenance processes. Position can be part-time or full-time. MVR certification preferred.\\n\\nPrimary Responsibilities\\n\\n  - Provide GAS2 selling experience for DIY customer visits and phone calls\\n  - Achieve personal sales goal and help store achieve its sales goals\\n  - Provide DIY services including battery installation, testing, wiper installs, etc.\\n  - Maintain store product and operational standards\\n  - Responsible for inventory processes including truck put away, shoot outs, cycle counts, Back stock, etc.\\n\\nSecondary Responsibilities\\n\\n  - Store Cleanliness including floors, bathrooms, facing, dusting, parking lot\\n  - General stocking including truck stocking and back stock\\n  - Safely deliver parts to customers as needed\\n\\nSuccess Factors\\n\\n  - Basic driving and navigation ability\\n  - Ability to use delivery board system\\n  - Friendly communication\\n  - Ability to locate and stock parts\\n  - Safety knowledge and skills\\n  - Operating inventory systems and store equipment\\n  - Parts and automotive system knowledge skills\\n  - Operating POS and Parts lookup systems\\n  - Expert at testing and diagnostic equipment for DIY service\\n\\nEssential Job Skills Necessary for Success as a Salesperson\\n\\n  - Speak and write English (Spanish a plus); communicate effectively and build strong relationships with customers, peers and management\\n  - Read and interpret documents such as safety rules, operating and maintenance instructions, parts catalogs, and procedure manuals\\n  - Use basic math accurately: add, subtract, multiply, and divide in all units of measure, using whole numbers, common fractions, and decimals\\n  - Ability to work an assortment of days, evenings, and weekends as needed\\n\\nPrior Experience that Sets a Salesperson up for Success\\n\\n2-3 years of successful sales experience in a diverse retail environment, providing superior customer experiences\\n\\nEducation\\n\\nHigh school diploma or general education degree (GED)\\n\\nCertificates, Licenses, Registrations\\n\\nNone\\n\\nPhysical Demands\\n\\nThe physical demands and work environment described here are representative of those that must be met by an employee to successfully perform the essential functions of this job, with or without reasonable accommodation. While performing the duties of this job, the employee will predominantly be walking or standing. The employee is required to be able to talk and hear, and use hands and fingers to handle or feel; reach with hands and arms; climb or balance; and stoop, kneel, crouch, or crawl. The employee must frequently lift and/or move up to 50 pounds and occasionally lift and/or move up to 100 pounds. Specific vision abilities required by this job include close vision, distance vision, color vision, depth perception, and ability to adjust focus.\\n\\nWork Environment\\n\\nThe work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job, with or without reasonable accommodation. While performing the duties of this job, the employee is usually working inside; however, they will occasionally be outside and exposed to various weather conditions while performing such tasks as installing batteries and wiper blades. The employee is also occasionally exposed to moving mechanical parts; high, precarious places; toxic or caustic chemicals; risk of electrical shock; explosives; and vibration. The noise level in the work environment is usually moderate.\\n\\nJob Requirements\\n\\nWe are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age national origin, religion, sexual orientation, gender identity, status as a veteran and basis of disability or any other federal, state or local protected class.'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(results[99]['JD_SCRUBBED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous 0 10 amod cars 11 4\n",
      "cars 11 4 nsubj shift 16 5\n",
      "shift 16 5 ROOT shift 16 5\n",
      "insurance 22 9 compound liability 32 9\n",
      "liability 32 9 dobj shift 16 5\n",
      "toward 42 6 prep shift 16 5\n",
      "manufacturers 49 13 pobj toward 42 6\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for token in doc:\n",
    "#     print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "#           [child for child in token.children])\n",
    "    print(token.text, token.idx, len(token), token.dep_, token.head.text, token.head.idx, len(token.head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1668940"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CREATING TRAINING FILE FOR STANFORD NLP FROM CONLL 2003 DATA\n",
    "lines = open('conll_train.txt').read().splitlines()[2:]\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    if len(lines[i].split(' ')) > 1:\n",
    "        lines[i] = str(lines[i].split(' ')[0]) + ' ' + str(lines[i].split(' ')[3])\n",
    "\n",
    "open('conll_train_stfd_nlp.txt','w').write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### COMMAND TO TRAIN STANFORDNLP NER: java -cp \"stanford-ner.jar:lib/*\" -mx4g edu.stanford.nlp.ie.crf.CRFClassifier -prop train/prop.txt\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "commands = '''\n",
    "cd /Users/nazih.bissat/Desktop/match.fracking/stanford-ner-tagger;\n",
    "java -cp \"stanford-ner.jar:lib/*\" -mx4g edu.stanford.nlp.ie.crf.CRFClassifier -prop train_conll/prop.txt\n",
    "'''\n",
    "\n",
    "process = Popen('/bin/bash', stdin=PIPE, stdout=PIPE)\n",
    "out, err = process.communicate(commands.encode('utf-8'))\n",
    "\n",
    "# full_entities = out.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPROCESSING TO FORMAT TRAINING DATA FOR SPACY\n",
    "lines = open('stanford-ner-tagger/train_conll/conll_train_stfd_nlp.tsv').read().splitlines()[2:]\n",
    "\n",
    "TRAIN_DATA = list()\n",
    "char_count = 0\n",
    "sent = ''\n",
    "ents = list()\n",
    "for i in range(len(lines)):\n",
    "    if len(lines[i].split(' ')) > 1:\n",
    "        word, entity_type = lines[i].split(' ')[0], lines[i].split(' ')[1]\n",
    "        sent += word + ' '\n",
    "        if entity_type != 'O':\n",
    "            ents.append((char_count, char_count + len(word), entity_type))\n",
    "        char_count += len(word) + 1\n",
    "    else:\n",
    "        TRAIN_DATA.append((sent, {'entities': ents}))\n",
    "        sent = ''\n",
    "        char_count = 0\n",
    "        ents = list()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('German call to boycott British lamb . ', {'entities': [(0, 6, 'I-MISC'), (23, 30, 'I-MISC')]}), ('Peter Blackburn ', {'entities': [(0, 5, 'I-PER'), (6, 15, 'I-PER')]}), ('BRUSSELS 1996-08-22 ', {'entities': [(0, 8, 'I-LOC')]}), ('The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep . ', {'entities': [(4, 12, 'I-ORG'), (13, 23, 'I-ORG'), (59, 65, 'I-MISC'), (94, 101, 'I-MISC')]}), (\"Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer . \", {'entities': [(0, 7, 'I-LOC'), (33, 41, 'I-ORG'), (42, 47, 'I-ORG'), (72, 78, 'I-PER'), (79, 88, 'I-PER'), (164, 171, 'I-LOC')]})]\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_DATA[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "Saved model to spaCy_conll_model\n",
      "Loading from spaCy_conll_model\n",
      "Entities []\n",
      "Tokens [('Orii', '', 0), ('-', '', 0), ('95/96', '', 0), ('group', '', 0), ('results', '', 0), ('.', '', 0)]\n",
      "Entities []\n",
      "Tokens [('7', '', 0), ('-', '', 0), ('Todd', '', 0), ('Martin', '', 0), ('(', '', 0), ('U.S.', '', 0), (')', '', 0), ('beat', '', 0), ('9', '', 0), ('-', '', 0), ('Cedric', '', 0), ('Pioline', '', 0), ('(', '', 0), ('France', '', 0), (')', '', 0), ('2', '', 0), ('-', '', 0), ('6', '', 0), ('6', '', 0), ('-', '', 0), ('2', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Results', '', 0), ('in', '', 0), ('the', '', 0), ('Malaysian', '', 0)]\n",
      "Entities []\n",
      "Tokens [('(', '', 0), ('players', '', 0), ('U.S.', '', 0), ('unless', '', 0), ('stated', '', 0), (')', '', 0), (':', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Oval', '', 0), ('on', '', 0), ('Sunday', '', 0), (':', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Palestinians', '', 0), ('to', '', 0), ('strike', '', 0), ('over', '', 0), ('Jerusalem', '', 0), ('demolition', '', 0), ('.', '', 0)]\n",
      "Entities []\n",
      "Tokens [('-------------------------', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Nigeria', '', 0), ('was', '', 0), ('suspended', '', 0), ('from', '', 0), ('the', '', 0), ('club', '', 0), ('of', '', 0), ('Britain', '', 0), ('and', '', 0), ('its', '', 0), ('former', '', 0), ('colonies', '', 0), ('in', '', 0), ('November', '', 0), ('after', '', 0), ('the', '', 0), ('hanging', '', 0), ('of', '', 0), ('nine', '', 0), ('minority', '', 0), ('rights', '', 0), ('activists', '', 0), ('for', '', 0), ('murder', '', 0), ('in', '', 0), ('spite', '', 0), ('of', '', 0), ('international', '', 0), ('pleas', '', 0), ('for', '', 0), ('clemency', '', 0), ('.', '', 0)]\n",
      "Entities []\n",
      "Tokens [('CHICAGO', '', 0), ('1996', '', 0), ('-', '', 0), ('08', '', 0), ('-', '', 0), ('28', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Czech', '', 0), ('coach', '', 0), ('in', '', 0), ('fatal', '', 0), ('crash', '', 0), ('in', '', 0), ('Austria', '', 0), ('.', '', 0)]\n",
      "Entities []\n",
      "Tokens [('-DOCSTART-', '', 0)]\n",
      "Entities []\n",
      "Tokens [('-DOCSTART-', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Leading', '', 0), ('stories', '', 0), ('in', '', 0), ('the', '', 0), ('Greek', '', 0), ('financial', '', 0), ('press', '', 0), (':', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Hopefully', '', 0), (',', '', 0), ('this', '', 0), ('will', '', 0), ('have', '', 0), ('been', '', 0), ('my', '', 0), ('nerves', '', 0), ('for', '', 0), ('the', '', 0), ('whole', '', 0), ('tournament', '', 0), ('.', '', 0), ('\"', '', 0)]\n",
      "Entities []\n",
      "Tokens [('TYPE', '', 0), ('straight', '', 0), ('bond', '', 0), ('ISSUE', '', 0), ('NO', '', 0), ('13', '', 0), ('AMT', '', 0), ('10', '', 0), ('bln', '', 0), ('yen', '', 0)]\n",
      "Entities []\n",
      "Tokens [('The', '', 0), ('challenge', '', 0), ('was', '', 0), ('just', '', 0), ('for', '', 0), ('me', '', 0), ('to', '', 0), ('keep', '', 0), ('playing', '', 0), ('my', '', 0), ('own', '', 0), ('game', '', 0), ('.', '', 0), ('\"', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Northamptonshire', '', 0), ('13', '', 0), ('2', '', 0), ('6', '', 0), ('5', '', 0), ('30', '', 0), ('43', '', 0), ('120', '', 0)]\n",
      "Entities []\n",
      "Tokens [('-DOCSTART-', '', 0)]\n",
      "Entities []\n",
      "Tokens [('William', '', 0), ('VanLandingham', '', 0), ('pitched', '', 0), ('eight', '', 0), ('scoreless', '', 0), ('innings', '', 0), ('and', '', 0), ('Glenallen', '', 0), ('Hill', '', 0), ('drove', '', 0), ('in', '', 0), ('the', '', 0), ('game', '', 0), (\"'s\", '', 0), ('only', '', 0), ('run', '', 0), ('with', '', 0), ('a', '', 0), ('first', '', 0), ('-', '', 0), ('inning', '', 0), ('single', '', 0), ('as', '', 0), ('the', '', 0), ('San', '', 0), ('Francisco', '', 0), ('Giants', '', 0), ('claimed', '', 0), ('a', '', 0), ('1', '', 0), ('-', '', 0), ('0', '', 0), ('victory', '', 0), ('over', '', 0), ('the', '', 0), ('Philadelphia', '', 0), ('Phillies', '', 0), ('on', '', 0), ('Monday', '', 0), ('.', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Shadab', '', 0), ('Kabir', '', 0), ('1', '', 0), ('0', '', 0), ('9', '', 0), ('0', '', 0), ('-', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Alberto', '', 0), ('Berasategui', '', 0), ('(', '', 0), ('Spain', '', 0), (')', '', 0), ('beat', '', 0), ('Cecil', '', 0), ('Mamiit', '', 0), ('(', '', 0), ('U.S.', '', 0), (')', '', 0), ('6', '', 0), ('-', '', 0), ('1', '', 0), ('6', '', 0), ('-', '', 0), ('4', '', 0), ('6', '', 0), ('-', '', 0), ('0', '', 0)]\n",
      "Entities []\n",
      "Tokens [('-DOCSTART-', '', 0)]\n",
      "Entities []\n",
      "Tokens [('1998', '', 0), ('575,000', '', 0)]\n",
      "Entities []\n",
      "Tokens [('-DOCSTART-', '', 0)]\n",
      "Entities []\n",
      "Tokens [('17', '', 0), ('.', '', 0), ('Paul', '', 0), ('Broadhurst', '', 0), ('172,580', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Vitesse', '', 0), ('Arnhem', '', 0), ('1', '', 0), ('Utrecht', '', 0), ('0', '', 0)]\n",
      "Entities []\n",
      "Tokens [('CYCLING', '', 0), ('-', '', 0), ('VAN', '', 0), ('HEESWIJK', '', 0), ('WINS', '', 0), ('TOUR', '', 0), ('OF', '', 0), ('NETHERLANDS', '', 0), ('SECOND', '', 0), ('STAGE', '', 0), ('.', '', 0)]\n",
      "Entities []\n",
      "Tokens [('matches', '', 0), ('to', '', 0), ('nil', '', 0), ('(', '', 0), ('with', '', 0), ('times', '', 0), ('for', '', 0), ('the', '', 0), ('last', '', 0), ('200', '', 0), ('metres', '', 0), ('of', '', 0), ('11.833', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Export', '', 0), ('demand', '', 0), ('was', '', 0), ('good', '', 0), ('but', '', 0), ('availability', '', 0), ('was', '', 0), ('limited', '', 0), ('.', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Olivier', '', 0), ('Panis', '', 0), ('(', '', 0), ('France', '', 0), (')', '', 0), ('Ligier', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Mar', '', 0), ('/', '', 0), ('Sep', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Hashimoto', '', 0), (',', '', 0), ('who', '', 0), ('arrived', '', 0), ('at', '', 0), ('11', '', 0), ('a.m.', '', 0), ('(', '', 0), ('1700', '', 0), ('GMT', '', 0), (')', '', 0), (',', '', 0), ('showed', '', 0), ('no', '', 0), ('sign', '', 0), ('of', '', 0), ('having', '', 0), ('felt', '', 0), ('the', '', 0), ('quake', '', 0), (',', '', 0), ('witnesses', '', 0), ('said', '', 0), ('.', '', 0)]\n",
      "Entities []\n",
      "Tokens [('-DOCSTART-', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Gencor', '', 0), ('swells', '', 0), ('profit', '', 0), ('despite', '', 0), ('setbacks', '', 0), ('.', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Jiul', '', 0), ('Petrosani', '', 0), ('1', '', 0), ('Dinamo', '', 0), ('Bucharest', '', 0), ('0', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Companion', '', 0), ('Marble', '', 0), ('posts', '', 0), ('1st', '', 0), ('final', '', 0), ('result', '', 0), ('.', '', 0)]\n",
      "Entities []\n",
      "Tokens [('After', '', 0), ('being', '', 0), ('fined', '', 0), ('$', '', 0), ('250,000', '', 0), ('by', '', 0), ('the', '', 0), ('sports', '', 0), ('governing', '', 0), ('body', '', 0), ('on', '', 0), ('Tuesday', '', 0), (',', '', 0), ('the', '', 0), ('British', '', 0), ('driver', '', 0), ('rolled', '', 0), ('his', '', 0), ('Subaru', '', 0), ('6.5', '', 0), ('km', '', 0), ('into', '', 0), ('stage', '', 0), ('six', '', 0), ('.', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Olsza', '', 0), ('is', '', 0), ('undaunted', '', 0), ('by', '', 0), ('the', '', 0), ('level', '', 0), ('of', '', 0), ('competition', '', 0), ('in', '', 0), ('the', '', 0), ('pros', '', 0), ('.', '', 0)]\n",
      "Entities []\n",
      "Tokens [('9/16', '', 0), ('-', '', 0), ('Luo', '', 0), ('Yigang', '', 0), ('(', '', 0), ('China', '', 0), (')', '', 0), ('beat', '', 0), ('Jason', '', 0), ('Wong', '', 0), ('(', '', 0), ('Malaysia', '', 0), (')', '', 0), ('15', '', 0), ('-', '', 0), ('5', '', 0), ('15', '', 0), ('-', '', 0), ('6', '', 0)]\n",
      "Entities []\n",
      "Tokens [('-DOCSTART-', '', 0)]\n",
      "Entities []\n",
      "Tokens [('8', '', 0), ('.', '', 0), ('Isaac', '', 0), ('Galvez', '', 0), ('-', '', 0), ('Lopez', '', 0), ('-', '', 0), ('Juan', '', 0), ('Llaneras', '', 0), ('(', '', 0), ('Spain', '', 0), (')', '', 0), ('11', '', 0)]\n",
      "Entities []\n",
      "Tokens [('JERUSALEM', '', 0), ('1996', '', 0), ('-', '', 0), ('08', '', 0), ('-', '', 0), ('25', '', 0)]\n",
      "Entities []\n",
      "Tokens [('We', '', 0), ('think', '', 0), ('that', '', 0), ('when', '', 0), ('the', '', 0), ('word', '', 0), ('about', '', 0), ('how', '', 0), ('things', '', 0), ('are', '', 0), ('going', '', 0), ('in', '', 0), ('Mahala', '', 0), ('which', '', 0), ('is', '', 0), ('about', '', 0), ('35', '', 0), ('minutes', '', 0), ('away', '', 0), ('by', '', 0), ('road', '', 0), ('reaches', '', 0), ('Zvornik', '', 0), ('that', '', 0), ('should', '', 0), ('help', '', 0), (',', '', 0), ('\"', '', 0), ('Marriner', '', 0), ('said', '', 0), ('.', '', 0)]\n",
      "Entities []\n",
      "Tokens [('Three', '', 0), ('dissidents', '', 0), ('and', '', 0), ('their', '', 0), ('translator', '', 0), ('were', '', 0), ('killed', '', 0), ('in', '', 0), ('the', '', 0), ('gangland', '', 0), ('-', '', 0), ('style', '', 0), ('machinegun', '', 0), ('attack', '', 0), ('.', '', 0)]\n",
      "Entities []\n",
      "Tokens [('The', '', 0), ('age', '', 0), ('of', '', 0), ('consent', '', 0), ('for', '', 0), ('heterosexual', '', 0), ('and', '', 0), ('homosexual', '', 0), ('sex', '', 0), ('in', '', 0), ('Albania', '', 0), ('is', '', 0), ('14', '', 0), ('.', '', 0)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities []\n",
      "Tokens [('3', '', 0), ('.', '', 0), ('Lance', '', 0), ('Armstrong', '', 0), ('(', '', 0), ('U.S.', '', 0), (')', '', 0), ('Motorola', '', 0)]\n",
      "Entities []\n",
      "Tokens [('LONDON', '', 0), ('1996', '', 0), ('-', '', 0), ('08', '', 0), ('-', '', 0), ('30', '', 0)]\n",
      "Entities []\n",
      "Tokens [('He', '', 0), ('praised', '', 0), ('Clinton', '', 0), ('for', '', 0), ('vetoing', '', 0), ('the', '', 0), ('Republican', '', 0), ('Congress', '', 0), (\"'\", '', 0), ('attempt', '', 0), ('to', '', 0), ('repeal', '', 0), ('parts', '', 0), ('of', '', 0), ('the', '', 0), ('Clean', '', 0), ('Air', '', 0), ('Act', '', 0), (',', '', 0), ('and', '', 0), ('criticized', '', 0), ('Republicans', '', 0), ('for', '', 0), ('\"', '', 0), ('slashing', '', 0), ('money', '', 0), ('to', '', 0), ('combat', '', 0), ('drugs', '', 0), ('in', '', 0), ('schools', '', 0), ('.', '', 0), ('\"', '', 0)]\n",
      "Entities []\n",
      "Tokens [('BRUSSELS', '', 0), ('1996', '', 0), ('-', '', 0), ('08', '', 0), ('-', '', 0), ('23', '', 0)]\n",
      "Entities []\n",
      "Tokens [('soccer', '', 0), ('matches', '', 0), ('played', '', 0), ('on', '', 0), ('Saturday', '', 0), (':', '', 0)]\n"
     ]
    }
   ],
   "source": [
    "### EXAMPLE CODE TO TRAIN SPACY MODEL\n",
    "import random\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "from pathlib import Path\n",
    "\n",
    "# TRAIN_DATA = [\n",
    "#      (\"Uber blew through $1 million a week\", {'entities': [(0, 4, 'ORG')]}),\n",
    "#      (\"Google rebrands its business apps\", {'entities': [(0, 6, \"ORG\")]})]\n",
    "\n",
    "output_dir = 'spaCy_conll_model'\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(20):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        nlp.update([text], [annotations], sgd=optimizer)\n",
    "\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)\n",
    "\n",
    "    # test the saved model\n",
    "    print(\"Loading from\", output_dir)\n",
    "    nlp2 = spacy.load(output_dir)\n",
    "    for text, _ in TRAIN_DATA[0:50]:\n",
    "        doc = nlp2(text)\n",
    "        if doc.ents != []:\n",
    "            print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(TRAIN_DATA[0:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nazih.bissat/anaconda3/envs/myenv1/lib/python3.6/site-packages/numpy/linalg/linalg.py:2257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  ret = sqrt(sqnorm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 30561.572949256533}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d61f24e69d16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spaCy_CONLL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-d61f24e69d16>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model, output_dir, n_iter)\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# dropout - make it harder to memorise data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# callable to update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     losses=losses)\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv1/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, docs, golds, drop, sgd, losses)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser._make_updates\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv1/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mcontinue_update\u001b[0;34m(gradient, sgd)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv1/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mfinish_update\u001b[0;34m(d_seqs_out, sgd)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_seqs_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0md_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbp_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_seqs_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0md_X\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv1/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mcontinue_update\u001b[0;34m(gradient, sgd)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv1/lib/python3.6/site-packages/thinc/neural/_classes/resnet.py\u001b[0m in \u001b[0;36mresidual_bwd\u001b[0;34m(d_output, sgd)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mresidual_bwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0md_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbp_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual_bwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv1/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mcontinue_update\u001b[0;34m(gradient, sgd)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mops.pyx\u001b[0m in \u001b[0;36mthinc.neural.ops.Ops.dropout.wrap_backprop.finish_update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv1/lib/python3.6/site-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36mfinish_update\u001b[0;34m(dy, sgd)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackprop_rescale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_dy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_dy_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_d_moments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0md_xhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdy\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msum_dy\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum_dy_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv1/lib/python3.6/site-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36mfinish_update\u001b[0;34m(gradient__BI, sgd)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_b\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgradient__BI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0md_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0md_G\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient__BI\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput__BI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv1/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### OR\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
    "existing model or a blank model.\n",
    "\n",
    "For more details, see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "\n",
    "# import multiprocessing\n",
    "\n",
    "# multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "# training data\n",
    "# TRAIN_DATA = [\n",
    "#     ('Who is Shaka Khan?', {\n",
    "#         'entities': [(7, 17, 'PERSON')]\n",
    "#     }),\n",
    "#     ('I like London and Berlin.', {\n",
    "#         'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
    "#     })\n",
    "# ]\n",
    "\n",
    "\n",
    "# @plac.annotations(\n",
    "#     model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "#     output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "#     n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "def main(model=None, output_dir=None, n_iter=5):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     plac.call(main)\n",
    "\n",
    "    # Expected output:\n",
    "    # Entities [('Shaka Khan', 'PERSON')]\n",
    "    # Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n",
    "    # ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
    "    # Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
    "    # Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n",
    "    # ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n",
    "    \n",
    "main(output_dir='spaCy_CONLL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU rejects German call to boycott British lamb . \n",
      "Peter Blackburn \n",
      "BRUSSELS 1996-08-22 \n",
      "The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep . \n",
      "Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer . \n",
      "\" We do n't support any such recommendation because we do n't see any grounds for it , \" the Commission 's chief spokesman Nikolaus van der Pas told a news briefing . \n"
     ]
    }
   ],
   "source": [
    "test = '''EU I-ORG\n",
    "rejects O\n",
    "German I-MISC\n",
    "call O\n",
    "to O\n",
    "boycott O\n",
    "British I-MISC\n",
    "lamb O\n",
    ". O\n",
    "\n",
    "Peter I-PER\n",
    "Blackburn I-PER\n",
    "\n",
    "BRUSSELS I-LOC\n",
    "1996-08-22 O\n",
    "\n",
    "The O\n",
    "European I-ORG\n",
    "Commission I-ORG\n",
    "said O\n",
    "on O\n",
    "Thursday O\n",
    "it O\n",
    "disagreed O\n",
    "with O\n",
    "German I-MISC\n",
    "advice O\n",
    "to O\n",
    "consumers O\n",
    "to O\n",
    "shun O\n",
    "British I-MISC\n",
    "lamb O\n",
    "until O\n",
    "scientists O\n",
    "determine O\n",
    "whether O\n",
    "mad O\n",
    "cow O\n",
    "disease O\n",
    "can O\n",
    "be O\n",
    "transmitted O\n",
    "to O\n",
    "sheep O\n",
    ". O\n",
    "\n",
    "Germany I-LOC\n",
    "'s O\n",
    "representative O\n",
    "to O\n",
    "the O\n",
    "European I-ORG\n",
    "Union I-ORG\n",
    "'s O\n",
    "veterinary O\n",
    "committee O\n",
    "Werner I-PER\n",
    "Zwingmann I-PER\n",
    "said O\n",
    "on O\n",
    "Wednesday O\n",
    "consumers O\n",
    "should O\n",
    "buy O\n",
    "sheepmeat O\n",
    "from O\n",
    "countries O\n",
    "other O\n",
    "than O\n",
    "Britain I-LOC\n",
    "until O\n",
    "the O\n",
    "scientific O\n",
    "advice O\n",
    "was O\n",
    "clearer O\n",
    ". O\n",
    "\n",
    "\" O\n",
    "We O\n",
    "do O\n",
    "n't O\n",
    "support O\n",
    "any O\n",
    "such O\n",
    "recommendation O\n",
    "because O\n",
    "we O\n",
    "do O\n",
    "n't O\n",
    "see O\n",
    "any O\n",
    "grounds O\n",
    "for O\n",
    "it O\n",
    ", O\n",
    "\" O\n",
    "the O\n",
    "Commission I-ORG\n",
    "'s O\n",
    "chief O\n",
    "spokesman O\n",
    "Nikolaus I-PER\n",
    "van I-PER\n",
    "der I-PER\n",
    "Pas I-PER\n",
    "told O\n",
    "a O\n",
    "news O\n",
    "briefing O\n",
    ". O'''\n",
    "\n",
    "text = ''\n",
    "for line in test.split('\\n'):\n",
    "    if line == '':\n",
    "        text += '\\n'\n",
    "    else:\n",
    "        text += line.split(' ')[0] + ' '\n",
    "    \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU\tI-ORG\tI-ORG\n",
      "rejects\tO\tO\n",
      "German\tI-MISC\tI-MISC\n",
      "call\tO\tO\n",
      "to\tO\tO\n",
      "boycott\tO\tO\n",
      "British\tI-MISC\tI-MISC\n",
      "lamb\tO\tO\n",
      ".\tO\tO\n",
      "\n",
      "Peter\tI-PER\tI-PER\n",
      "Blackburn\tI-PER\tI-PER\n",
      "\n",
      "BRUSSELS\tI-LOC\tI-LOC\n",
      "1996-08-22\tO\tO\n",
      "\n",
      "The\tO\tO\n",
      "European\tI-ORG\tI-ORG\n",
      "Commission\tI-ORG\tI-ORG\n",
      "said\tO\tO\n",
      "on\tO\tO\n",
      "Thursday\tO\tO\n",
      "it\tO\tO\n",
      "disagreed\tO\tO\n",
      "with\tO\tO\n",
      "German\tI-MISC\tI-MISC\n",
      "advice\tO\tO\n",
      "to\tO\tO\n",
      "consumers\tO\tO\n",
      "to\tO\tO\n",
      "shun\tO\tO\n",
      "British\tI-MISC\tI-MISC\n",
      "lamb\tO\tO\n",
      "until\tO\tO\n",
      "scientists\tO\tO\n",
      "determine\tO\tO\n",
      "whether\tO\tO\n",
      "mad\tO\tO\n",
      "cow\tO\tO\n",
      "disease\tO\tO\n",
      "can\tO\tO\n",
      "be\tO\tO\n",
      "transmitted\tO\tO\n",
      "to\tO\tO\n",
      "sheep\tO\tO\n",
      ".\tO\tO\n",
      "\n",
      "Germany\tI-LOC\tI-LOC\n",
      "'s\tO\tO\n",
      "representative\tO\tO\n",
      "to\tO\tO\n",
      "the\tO\tO\n",
      "European\tI-ORG\tI-ORG\n",
      "Union\tI-ORG\tI-ORG\n",
      "'s\tO\tO\n",
      "veterinary\tO\tO\n",
      "committee\tO\tO\n",
      "Werner\tI-PER\tI-PER\n",
      "Zwingmann\tI-PER\tI-PER\n",
      "said\tO\tO\n",
      "on\tO\tO\n",
      "Wednesday\tO\tO\n",
      "consumers\tO\tO\n",
      "should\tO\tO\n",
      "buy\tO\tO\n",
      "sheepmeat\tO\tO\n",
      "from\tO\tO\n",
      "countries\tO\tO\n",
      "other\tO\tO\n",
      "than\tO\tO\n",
      "Britain\tI-LOC\tI-LOC\n",
      "until\tO\tO\n",
      "the\tO\tO\n",
      "scientific\tO\tO\n",
      "advice\tO\tO\n",
      "was\tO\tO\n",
      "clearer\tO\tO\n",
      ".\tO\tO\n",
      "\n",
      "\"\tO\tO\n",
      "We\tO\tO\n",
      "do\tO\tO\n",
      "n't\tO\tO\n",
      "support\tO\tO\n",
      "any\tO\tO\n",
      "such\tO\tO\n",
      "recommendation\tO\tO\n",
      "because\tO\tO\n",
      "we\tO\tO\n",
      "do\tO\tO\n",
      "n't\tO\tO\n",
      "see\tO\tO\n",
      "any\tO\tO\n",
      "grounds\tO\tO\n",
      "for\tO\tO\n",
      "it\tO\tO\n",
      ",\tO\tO\n",
      "\"\tO\tO\n",
      "the\tO\tO\n",
      "Commission\tI-ORG\tI-ORG\n",
      "'s\tO\tO\n",
      "chief\tO\tO\n",
      "spokesman\tO\tO\n",
      "Nikolaus\tI-PER\tI-PER\n",
      "van\tI-PER\tI-PER\n",
      "der\tI-PER\tI-PER\n",
      "Pas\tI-PER\tI-PER\n",
      "told\tO\tO\n",
      "a\tO\tO\n",
      "news\tO\tO\n",
      "briefing\tO\tO\n",
      ".\tO\tO\n",
      "\n",
      "He\tO\tO\n",
      "said\tO\tO\n",
      "further\tO\tO\n",
      "scientific\tO\tO\n",
      "study\tO\tO\n",
      "was\tO\tO\n",
      "required\tO\tO\n",
      "and\tO\tO\n",
      "if\tO\tO\n",
      "it\tO\tO\n",
      "was\tO\tO\n",
      "found\tO\tO\n",
      "that\tO\tO\n",
      "action\tO\tO\n",
      "was\tO\tO\n",
      "needed\tO\tO\n",
      "it\tO\tO\n",
      "should\tO\tO\n",
      "be\tO\tO\n",
      "taken\tO\tO\n",
      "by\tO\tO\n",
      "the\tO\tO\n",
      "European\tI-ORG\tI-ORG\n",
      "Union\tI-ORG\tI-ORG\n",
      ".\tO\tO\n",
      "\n",
      "He\tO\tO\n",
      "said\tO\tO\n",
      "a\tO\tO\n",
      "proposal\tO\tO\n",
      "last\tO\tO\n",
      "month\tO\tO\n",
      "by\tO\tO\n",
      "EU\tI-ORG\tI-ORG\n",
      "Farm\tO\tO\n",
      "Commissioner\tO\tO\n",
      "Franz\tI-PER\tI-PER\n",
      "Fischler\tI-PER\tI-PER\n",
      "to\tO\tO\n",
      "ban\tO\tO\n",
      "sheep\tO\tO\n",
      "brains\tO\tO\n",
      ",\tO\tO\n",
      "spleens\tO\tO\n",
      "and\tO\tO\n",
      "spinal\tO\tO\n",
      "cords\tO\tO\n",
      "from\tO\tO\n",
      "the\tO\tO\n",
      "human\tO\tO\n",
      "and\tO\tO\n",
      "animal\tO\tO\n",
      "food\tO\tO\n",
      "chains\tO\tO\n",
      "was\tO\tO\n",
      "a\tO\tO\n",
      "highly\tO\tO\n",
      "specific\tO\tO\n",
      "and\tO\tO\n",
      "precautionary\tO\tO\n",
      "move\tO\tO\n",
      "to\tO\tO\n",
      "protect\tO\tO\n",
      "human\tO\tO\n",
      "health\tO\tO\n",
      ".\tO\tO\n",
      "\n",
      "Fischler\tI-PER\tI-PER\n",
      "proposed\tO\tO\n",
      "EU-wide\tI-MISC\tI-MISC\n",
      "measures\tO\tO\n",
      "after\tO\tO\n",
      "reports\tO\tO\n",
      "from\tO\tO\n",
      "Britain\tI-LOC\tI-LOC\n",
      "and\tO\tO\n",
      "France\tI-LOC\tI-LOC\n",
      "that\tO\tO\n",
      "under\tO\tO\n",
      "laboratory\tO\tO\n",
      "conditions\tO\tO\n",
      "sheep\tO\tO\n",
      "could\tO\tO\n",
      "contract\tO\tO\n",
      "Bovine\tI-MISC\tI-MISC\n",
      "Spongiform\tI-MISC\tI-MISC\n",
      "Encephalopathy\tI-MISC\tI-MISC\n",
      "(\tO\tO\n",
      "BSE\tI-MISC\tI-MISC\n",
      ")\tO\tO\n",
      "--\tO\tO\n",
      "mad\tO\tO\n",
      "cow\tO\tO\n",
      "disease\tO\tO\n",
      ".\tO\tO\n",
      "\n",
      "But\tO\tO\n",
      "Fischler\tI-PER\tI-PER\n",
      "agreed\tO\tO\n",
      "to\tO\tO\n",
      "review\tO\tO\n",
      "his\tO\tO\n",
      "proposal\tO\tO\n",
      "after\tO\tO\n",
      "the\tO\tO\n",
      "EU\tI-ORG\tI-ORG\n",
      "'s\tO\tO\n",
      "standing\tO\tO\n",
      "veterinary\tO\tO\n",
      "committee\tO\tO\n",
      ",\tO\tO\n",
      "mational\tO\tO\n",
      "animal\tO\tO\n",
      "health\tO\tO\n",
      "officials\tO\tO\n",
      ",\tO\tO\n",
      "questioned\tO\tO\n",
      "if\tO\tO\n",
      "such\tO\tO\n",
      "action\tO\tO\n",
      "was\tO\tO\n",
      "justified\tO\tO\n",
      "as\tO\tO\n",
      "there\tO\tO\n",
      "was\tO\tO\n",
      "only\tO\tO\n",
      "a\tO\tO\n",
      "slight\tO\tO\n",
      "risk\tO\tO\n",
      "to\tO\tO\n",
      "human\tO\tO\n",
      "health\tO\tO\n",
      ".\tO\tO\n",
      "\n",
      "Spanish\tI-MISC\tI-MISC\n",
      "Farm\tO\tO\n",
      "Minister\tO\tO\n",
      "Loyola\tI-PER\tI-PER\n",
      "de\tI-PER\tI-PER\n",
      "Palacio\tI-PER\tI-PER\n",
      "had\tO\tO\n",
      "earlier\tO\tO\n",
      "accused\tO\tO\n",
      "Fischler\tI-PER\tI-PER\n",
      "at\tO\tO\n",
      "an\tO\tO\n",
      "EU\tI-ORG\tI-ORG\n",
      "farm\tO\tO\n",
      "ministers\tO\tO\n",
      "'\tO\tO\n",
      "meeting\tO\tO\n",
      "of\tO\tO\n",
      "causing\tO\tO\n",
      "unjustified\tO\tO\n",
      "alarm\tO\tO\n",
      "through\tO\tO\n",
      "\"\tO\tO\n",
      "dangerous\tO\tO\n",
      "generalisation\tO\tO\n",
      ".\tO\tO\n",
      "\"\tO\tO\n",
      "\n",
      ".\tO\tO\n",
      "\n",
      "Only\tO\tO\n",
      "France\tI-LOC\tI-LOC\n",
      "and\tO\tO\n",
      "Britain\tI-LOC\tI-LOC\n",
      "backed\tO\tO\n",
      "Fischler\tI-PER\tI-PER\n",
      "'s\tO\tO\n",
      "proposal\tO\tO\n",
      ".\tO\tO\n",
      "\n",
      "The\tO\tO\n",
      "EU\tI-ORG\tI-ORG\n",
      "'s\tO\tO\n",
      "scientific\tO\tO\n",
      "veterinary\tO\tO\n",
      "and\tO\tO\n",
      "multidisciplinary\tO\tO\n",
      "committees\tO\tO\n",
      "are\tO\tO\n",
      "due\tO\tO\n",
      "to\tO\tO\n",
      "re-examine\tO\tO\n",
      "the\tO\tO\n",
      "issue\tO\tO\n",
      "early\tO\tO\n",
      "next\tO\tO\n",
      "month\tO\tO\n",
      "and\tO\tO\n",
      "make\tO\tO\n",
      "recommendations\tO\tO\n",
      "to\tO\tO\n",
      "the\tO\tO\n",
      "senior\tO\tO\n",
      "veterinary\tO\tO\n",
      "officials\tO\tO\n",
      ".\tO\tO\n",
      "\n",
      "Sheep\tO\tO\n",
      "have\tO\tO\n",
      "long\tO\tO\n",
      "been\tO\tO\n",
      "known\tO\tO\n",
      "to\tO\tO\n",
      "contract\tO\tO\n",
      "scrapie\tO\tO\n",
      ",\tO\tO\n",
      "a\tO\tO\n",
      "brain-wasting\tO\tO\n",
      "disease\tO\tO\n",
      "similar\tO\tO\n",
      "to\tO\tO\n",
      "BSE\tI-MISC\tI-MISC\n",
      "which\tO\tO\n",
      "is\tO\tO\n",
      "believed\tO\tO\n",
      "to\tO\tO\n",
      "have\tO\tO\n",
      "been\tO\tO\n",
      "transferred\tO\tO\n",
      "to\tO\tO\n",
      "cattle\tO\tO\n",
      "through\tO\tO\n",
      "feed\tO\tO\n",
      "containing\tO\tO\n",
      "animal\tO\tO\n",
      "waste\tO\tO\n",
      ".\tO\tO\n",
      "\n",
      "British\tI-MISC\tI-MISC\n",
      "farmers\tO\tO\n",
      "denied\tO\tO\n",
      "on\tO\tO\n",
      "Thursday\tO\tO\n",
      "there\tO\tO\n",
      "was\tO\tO\n",
      "any\tO\tO\n",
      "danger\tO\tO\n",
      "to\tO\tO\n",
      "human\tO\tO\n",
      "health\tO\tO\n",
      "from\tO\tO\n",
      "their\tO\tO\n",
      "sheep\tO\tO\n",
      ",\tO\tO\n",
      "but\tO\tO\n",
      "expressed\tO\tO\n",
      "concern\tO\tO\n",
      "that\tO\tO\n",
      "German\tI-MISC\tI-MISC\n",
      "government\tO\tO\n",
      "advice\tO\tO\n",
      "to\tO\tO\n",
      "consumers\tO\tO\n",
      "to\tO\tO\n",
      "avoid\tO\tO\n",
      "British\tI-MISC\tI-MISC\n",
      "lamb\tO\tO\n",
      "might\tO\tO\n",
      "influence\tO\tO\n",
      "consumers\tO\tO\n",
      "across\tO\tO\n",
      "Europe\tI-LOC\tI-LOC\n",
      ".\tO\tO\n",
      "\n",
      "\"\tO\tO\n",
      "What\tO\tO\n",
      "we\tO\tO\n",
      "have\tO\tO\n",
      "to\tO\tO\n",
      "be\tO\tO\n",
      "extremely\tO\tO\n",
      "careful\tO\tO\n",
      "of\tO\tO\n",
      "is\tO\tO\n",
      "how\tO\tO\n",
      "other\tO\tO\n",
      "countries\tO\tO\n",
      "are\tO\tO\n",
      "going\tO\tO\n",
      "to\tO\tO\n",
      "take\tO\tO\n",
      "Germany\tI-LOC\tI-LOC\n",
      "'s\tO\tO\n",
      "lead\tO\tO\n",
      ",\tO\tO\n",
      "\"\tO\tO\n",
      "Welsh\tI-ORG\tI-ORG\n",
      "National\tI-ORG\tI-ORG\n",
      "Farmers\tI-ORG\tI-ORG\n",
      "'\tI-ORG\tI-ORG\n",
      "Union\tI-ORG\tI-ORG\n",
      "(\tO\tO\n",
      "NFU\tI-ORG\tI-ORG\n",
      ")\tO\tO\n",
      "chairman\tO\tO\n",
      "John\tI-PER\tI-PER\n",
      "Lloyd\tI-PER\tI-PER\n",
      "Jones\tI-PER\tI-PER\n",
      "said\tO\tO\n",
      "on\tO\tO\n",
      "BBC\tI-ORG\tI-ORG\n",
      "radio\tI-ORG\tI-ORG\n",
      ".\tO\tO\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## THIS CELL HAS CODE TO TEST A TRAINED NER MODEL\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "commands = '''\n",
    "cd /Users/nazih.bissat/Desktop/match.fracking/stanford-ner-tagger;\n",
    "java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier conll-train-ner-model.ser.gz -testFile test.tsv\n",
    "'''\n",
    "\n",
    "process = Popen('/bin/bash', stdin=PIPE, stdout=PIPE)\n",
    "out, err = process.communicate(commands.encode('utf-8'))\n",
    "\n",
    "full_entities = out.decode('utf-8')\n",
    "print(out.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train StanfordNLP NER model from brat\n",
    "train_data = ''\n",
    "\n",
    "for filename in os.listdir('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/NER/StanfordNLP'):\n",
    "    for i in np.arange(1, 101, 1):\n",
    "        if filename.startswith(\"posting\" + str(i)): \n",
    "            # print(os.path.join(directory, filename))\n",
    "            continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ''\n",
    "\n",
    "posting = '''Job Description\n",
    "\n",
    "HUB Order Puller\n",
    "\n",
    "At Advance Auto Parts, a HUB Order Puller is primarily responsible for pulling HUB customer orders.\n",
    "\n",
    "ESSENTIAL DUTIES AND RESPONSIBILITIES include the following. Other duties may be assigned.\n",
    "\n",
    "  - Pull customer orders and prepare for delivery.\n",
    "  - Assist with the receiving and unloading of replenishment stock.\n",
    "  - Store replenishment stock in proper location.\n",
    "  - Accurately and legibly complete all paperwork related to the task of an order puller.\n",
    "  - Properly maintain equipment and report any problems on a timely basis; keep work area clean and neat.\n",
    "  - Operate at a professional level of efficiency in order not to breech company security.\n",
    "  - Work with store management team to complete MAXI changes in parts department.\n",
    "  - Comply with all federal, state and local laws.\n",
    "  - Comply with all company policies and procedures.\n",
    "  - Complete all required training materials, attend all scheduled company meetings.\n",
    "\n",
    "Job Requirements\n",
    "\n",
    "HUB Order Puller\n",
    "\n",
    "QUALIFICATIONS\n",
    "\n",
    "To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\n",
    "\n",
    "  - Able to use a hand truck and pallet jack.\n",
    "  - Able to follow detailed instructions.\n",
    "  - Able to work efficiently with others both within and outside the store.\n",
    "  - Able to perform successfully with minimum supervision.\n",
    "  - Maintain a good personal appearance.\n",
    "  - Scheduling factors necessitate that the individual should be able to come to work on short notice, and/or work flexible hours including nights and weekends.\n",
    "\n",
    "EDUCATION and/or EXPERIENCE\n",
    "\n",
    "  - High school diploma or GED equivalent; or one to three months related experience and/or training; or equivalent combination or education and experience.\n",
    "\n",
    "LANGUAGE SKILLS\n",
    "\n",
    "  - Ability to read and comprehend simple instruction, short correspondence, and memos.\n",
    "  - Ability to write simple correspondence.\n",
    "  - Ability to effectively present information in one on one and small group situations to customer, clients and other Team Members of the organization.\n",
    "\n",
    "MATHEMATICAL SKILLS\n",
    "\n",
    "  - Ability to add, subtract, multiply, and divide in all units of measure, using whole numbers, common fractions, and decimals.\n",
    "  - Ability to compute rates, ratios, percentages and interprets bar graphs.\n",
    "\n",
    "REASONING ABILITY\n",
    "\n",
    "  - Ability to define problems collects and evaluates information, establish facts and draw valid conclusions.\n",
    "  - Ability to interpret Advance policies and'''\n",
    "\n",
    "anns = '''T1\tOrganization 732 736\tMAXI\n",
    "T2\tTitle 1144 1158\trepresentative\n",
    "T3\tDuration 1825 1828\tone\n",
    "T4\tDuration 1832 1844\tthree months\n",
    "T5\tNumber 2136 2139\tone\n",
    "T6\tNumber 2143 2146\tone'''\n",
    "\n",
    "def insertPeriod(position, mystring):\n",
    "    longi = len(mystring)\n",
    "    mystring   =  mystring[:position] + '.' + mystring[position:] \n",
    "    return mystring \n",
    "\n",
    "posting = posting.strip()\n",
    "posting = re.sub(r'(\\n-)', '\\n', posting)\n",
    "posting = re.sub(r'(\\n  -)', '\\n', posting)\n",
    "posting = re.sub(r'(\\n  -)', '\\n', posting)\n",
    "posting = re.sub(r'(\\n  )', '\\n', posting)\n",
    "    \n",
    "indices = [x.start() for x in re.finditer(r'\\n', posting)]\n",
    "\n",
    "counter = 0\n",
    "for i in indices:\n",
    "    if posting[i+counter-1] != '.':\n",
    "        if posting[i+counter-1] != '\\n':\n",
    "            posting = insertPeriod(i+counter, posting)\n",
    "            counter += 1\n",
    "            \n",
    "tokens = posting.split(' ')\n",
    "\n",
    "ents = [ann.split('\\t')[1].split(' ') for ann in anns.split('\\n')]\n",
    "\n",
    "counter = 0\n",
    "for t in tokens:\n",
    "    ent = False\n",
    "    for i in ents:\n",
    "        if (counter == int(i[1])):\n",
    "            train_data += t + ' ' + i[0] + '\\n'\n",
    "            ent = True\n",
    "    if ent == False:\n",
    "        train_data += t + ' ' + 'O' + '\\n'\n",
    "    counter += len(t) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Job', 'Description.\\n\\nHUB', 'Order', 'Puller.\\n\\nAt', 'Advance', 'Auto', 'Parts,', 'a', 'HUB', 'Order', 'Puller', 'is', 'primarily', 'responsible', 'for', 'pulling', 'HUB', 'customer', 'orders.\\n\\nESSENTIAL', 'DUTIES', 'AND', 'RESPONSIBILITIES', 'include', 'the', 'following.', 'Other', 'duties', 'may', 'be', 'assigned.\\n\\n', 'Pull', 'customer', 'orders', 'and', 'prepare', 'for', 'delivery.\\n', 'Assist', 'with', 'the', 'receiving', 'and', 'unloading', 'of', 'replenishment', 'stock.\\n', 'Store', 'replenishment', 'stock', 'in', 'proper', 'location.\\n', 'Accurately', 'and', 'legibly', 'complete', 'all', 'paperwork', 'related', 'to', 'the', 'task', 'of', 'an', 'order', 'puller.\\n', 'Properly', 'maintain', 'equipment', 'and', 'report', 'any', 'problems', 'on', 'a', 'timely', 'basis;', 'keep', 'work', 'area', 'clean', 'and', 'neat.\\n', 'Operate', 'at', 'a', 'professional', 'level', 'of', 'efficiency', 'in', 'order', 'not', 'to', 'breech', 'company', 'security.\\n', 'Work', 'with', 'store', 'management', 'team', 'to', 'complete', 'MAXI', 'changes', 'in', 'parts', 'department.\\n', 'Comply', 'with', 'all', 'federal,', 'state', 'and', 'local', 'laws.\\n', 'Comply', 'with', 'all', 'company', 'policies', 'and', 'procedures.\\n', 'Complete', 'all', 'required', 'training', 'materials,', 'attend', 'all', 'scheduled', 'company', 'meetings.\\n\\nJob', 'Requirements.\\n\\nHUB', 'Order', 'Puller.\\n\\nQUALIFICATIONS.\\n\\nTo', 'perform', 'this', 'job', 'successfully,', 'an', 'individual', 'must', 'be', 'able', 'to', 'perform', 'each', 'essential', 'duty', 'satisfactorily.', 'The', 'requirements', 'listed', 'are', 'representative', 'of', 'the', 'knowledge,', 'skill,', 'and/or', 'ability', 'required.', 'Reasonable', 'accommodations', 'may', 'be', 'made', 'to', 'enable', 'individuals', 'with', 'disabilities', 'to', 'perform', 'the', 'essential', 'functions.\\n\\n', 'Able', 'to', 'use', 'a', 'hand', 'truck', 'and', 'pallet', 'jack.\\n', 'Able', 'to', 'follow', 'detailed', 'instructions.\\n', 'Able', 'to', 'work', 'efficiently', 'with', 'others', 'both', 'within', 'and', 'outside', 'the', 'store.\\n', 'Able', 'to', 'perform', 'successfully', 'with', 'minimum', 'supervision.\\n', 'Maintain', 'a', 'good', 'personal', 'appearance.\\n', 'Scheduling', 'factors', 'necessitate', 'that', 'the', 'individual', 'should', 'be', 'able', 'to', 'come', 'to', 'work', 'on', 'short', 'notice,', 'and/or', 'work', 'flexible', 'hours', 'including', 'nights', 'and', 'weekends.\\n\\nEDUCATION', 'and/or', 'EXPERIENCE.\\n\\n', 'High', 'school', 'diploma', 'or', 'GED', 'equivalent;', 'or', 'one', 'to', 'three', 'months', 'related', 'experience', 'and/or', 'training;', 'or', 'equivalent', 'combination', 'or', 'education', 'and', 'experience.\\n\\nLANGUAGE', 'SKILLS.\\n\\n', 'Ability', 'to', 'read', 'and', 'comprehend', 'simple', 'instruction,', 'short', 'correspondence,', 'and', 'memos.\\n', 'Ability', 'to', 'write', 'simple', 'correspondence.\\n', 'Ability', 'to', 'effectively', 'present', 'information', 'in', 'one', 'on', 'one', 'and', 'small', 'group', 'situations', 'to', 'customer,', 'clients', 'and', 'other', 'Team', 'Members', 'of', 'the', 'organization.\\n\\nMATHEMATICAL', 'SKILLS.\\n\\n', 'Ability', 'to', 'add,', 'subtract,', 'multiply,', 'and', 'divide', 'in', 'all', 'units', 'of', 'measure,', 'using', 'whole', 'numbers,', 'common', 'fractions,', 'and', 'decimals.\\n', 'Ability', 'to', 'compute', 'rates,', 'ratios,', 'percentages', 'and', 'interprets', 'bar', 'graphs.\\n\\nREASONING', 'ABILITY.\\n\\n', 'Ability', 'to', 'define', 'problems', 'collects', 'and', 'evaluates', 'information,', 'establish', 'facts', 'and', 'draw', 'valid', 'conclusions.\\n', 'Ability', 'to', 'interpret', 'Advance', 'policies', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732\n",
      "MAXI\n",
      "1144\n",
      "representative\n",
      "1825\n",
      "one\n",
      "1832\n",
      "three\n",
      "2136\n",
      "one\n",
      "2143\n",
      "one\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for t in tokens:\n",
    "    for i in ents:\n",
    "        if (counter == int(i[0])):\n",
    "            print(i[0])\n",
    "            print(t)\n",
    "    counter += len(t) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file pair: posting16.ann and posting16.txt\n",
      "Processed file pair: posting17.ann and posting17.txt\n",
      "Processed file pair: posting29.ann and posting29.txt\n",
      "Processed file pair: posting15.ann and posting15.txt\n",
      "Processed file pair: posting100.ann and posting100.txt\n",
      "Processed file pair: posting14.ann and posting14.txt\n",
      "Processed file pair: posting28.ann and posting28.txt\n",
      "Processed file pair: posting10.ann and posting10.txt\n",
      "Processed file pair: posting38.ann and posting38.txt\n",
      "Processed file pair: posting39.ann and posting39.txt\n",
      "Processed file pair: posting11.ann and posting11.txt\n",
      "Processed file pair: posting13.ann and posting13.txt\n",
      "Processed file pair: posting12.ann and posting12.txt\n",
      "Processed file pair: posting49.ann and posting49.txt\n",
      "Processed file pair: posting61.ann and posting61.txt\n",
      "Processed file pair: posting75.ann and posting75.txt\n",
      "Processed file pair: posting74.ann and posting74.txt\n",
      "Processed file pair: posting60.ann and posting60.txt\n",
      "Processed file pair: posting48.ann and posting48.txt\n",
      "Processed file pair: posting89.ann and posting89.txt\n",
      "Processed file pair: posting76.ann and posting76.txt\n",
      "Processed file pair: posting62.ann and posting62.txt\n",
      "Processed file pair: posting63.ann and posting63.txt\n",
      "Processed file pair: posting77.ann and posting77.txt\n",
      "Processed file pair: posting88.ann and posting88.txt\n",
      "Processed file pair: posting98.ann and posting98.txt\n",
      "Processed file pair: posting73.ann and posting73.txt\n",
      "Processed file pair: posting67.ann and posting67.txt\n",
      "Processed file pair: posting9.ann and posting9.txt\n",
      "Processed file pair: posting8.ann and posting8.txt\n",
      "Processed file pair: posting66.ann and posting66.txt\n",
      "Processed file pair: posting72.ann and posting72.txt\n",
      "Processed file pair: posting99.ann and posting99.txt\n",
      "Processed file pair: posting64.ann and posting64.txt\n",
      "Processed file pair: posting70.ann and posting70.txt\n",
      "Processed file pair: posting58.ann and posting58.txt\n",
      "Processed file pair: posting59.ann and posting59.txt\n",
      "Processed file pair: posting71.ann and posting71.txt\n",
      "Processed file pair: posting65.ann and posting65.txt\n",
      "Processed file pair: posting83.ann and posting83.txt\n",
      "Processed file pair: posting97.ann and posting97.txt\n",
      "Processed file pair: posting68.ann and posting68.txt\n",
      "Processed file pair: posting40.ann and posting40.txt\n",
      "Processed file pair: posting54.ann and posting54.txt\n",
      "Processed file pair: posting6.ann and posting6.txt\n",
      "Processed file pair: posting7.ann and posting7.txt\n",
      "Processed file pair: posting55.ann and posting55.txt\n",
      "Processed file pair: posting41.ann and posting41.txt\n",
      "Processed file pair: posting69.ann and posting69.txt\n",
      "Processed file pair: posting96.ann and posting96.txt\n",
      "Processed file pair: posting82.ann and posting82.txt\n",
      "Processed file pair: posting94.ann and posting94.txt\n",
      "Processed file pair: posting80.ann and posting80.txt\n",
      "Processed file pair: posting57.ann and posting57.txt\n",
      "Processed file pair: posting43.ann and posting43.txt\n",
      "Processed file pair: posting5.ann and posting5.txt\n",
      "Processed file pair: posting4.ann and posting4.txt\n",
      "Processed file pair: posting42.ann and posting42.txt\n",
      "Processed file pair: posting56.ann and posting56.txt\n",
      "Processed file pair: posting81.ann and posting81.txt\n",
      "Processed file pair: posting95.ann and posting95.txt\n",
      "Processed file pair: posting91.ann and posting91.txt\n",
      "Processed file pair: posting85.ann and posting85.txt\n",
      "Processed file pair: posting52.ann and posting52.txt\n",
      "Processed file pair: posting46.ann and posting46.txt\n",
      "Processed file pair: posting1.ann and posting1.txt\n",
      "Processed file pair: posting47.ann and posting47.txt\n",
      "Processed file pair: posting53.ann and posting53.txt\n",
      "Processed file pair: posting84.ann and posting84.txt\n",
      "Processed file pair: posting90.ann and posting90.txt\n",
      "Processed file pair: posting86.ann and posting86.txt\n",
      "Processed file pair: posting92.ann and posting92.txt\n",
      "Processed file pair: posting45.ann and posting45.txt\n",
      "Processed file pair: posting51.ann and posting51.txt\n",
      "Processed file pair: posting79.ann and posting79.txt\n",
      "Processed file pair: posting3.ann and posting3.txt\n",
      "Processed file pair: posting2.ann and posting2.txt\n",
      "Processed file pair: posting78.ann and posting78.txt\n",
      "Processed file pair: posting50.ann and posting50.txt\n",
      "Processed file pair: posting44.ann and posting44.txt\n",
      "Processed file pair: posting93.ann and posting93.txt\n",
      "Processed file pair: posting87.ann and posting87.txt\n",
      "Processed file pair: posting23.ann and posting23.txt\n",
      "Processed file pair: posting37.ann and posting37.txt\n",
      "Processed file pair: posting36.ann and posting36.txt\n",
      "Processed file pair: posting22.ann and posting22.txt\n",
      "Processed file pair: posting34.ann and posting34.txt\n",
      "Processed file pair: posting20.ann and posting20.txt\n",
      "Processed file pair: posting21.ann and posting21.txt\n",
      "Processed file pair: posting35.ann and posting35.txt\n",
      "Processed file pair: posting31.ann and posting31.txt\n",
      "Processed file pair: posting25.ann and posting25.txt\n",
      "Processed file pair: posting19.ann and posting19.txt\n",
      "Processed file pair: posting18.ann and posting18.txt\n",
      "Processed file pair: posting24.ann and posting24.txt\n",
      "Processed file pair: posting30.ann and posting30.txt\n",
      "Processed file pair: posting26.ann and posting26.txt\n",
      "Processed file pair: posting32.ann and posting32.txt\n",
      "Processed file pair: posting33.ann and posting33.txt\n",
      "Processed file pair: posting27.ann and posting27.txt\n"
     ]
    }
   ],
   "source": [
    "# A python script to turn annotated data in standoff format (brat annotation tool) to the formats expected by Stanford NER and Relation Extractor models\n",
    "# - NER format based on: http://nlp.stanford.edu/software/crf-faq.html#a\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "DEFAULT_OTHER_ANNO = 'O'\n",
    "DATA_DIRECTORY = '/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/NER/StanfordNLP'\n",
    "OUTPUT_DIRECTORY = 'stanford-nlp-train-model'\n",
    "\n",
    "NER_TRAINING_DATA_OUTPUT_PATH = join(OUTPUT_DIRECTORY, 'ner-crf-training-data.tsv')\n",
    "\n",
    "if os.path.exists(OUTPUT_DIRECTORY):\n",
    "    if os.path.exists(NER_TRAINING_DATA_OUTPUT_PATH):\n",
    "        os.remove(NER_TRAINING_DATA_OUTPUT_PATH)\n",
    "else:\n",
    "    os.makedirs(OUTPUT_DIRECTORY)\n",
    "\n",
    "sentence_count = 0\n",
    "# nlp = StanfordCoreNLP(CORENLP_SERVER_ADDRESS)\n",
    "startup_corenlp_server()\n",
    "\n",
    "# looping through .ann files in the data directory\n",
    "ann_data_files = [f for f in listdir(DATA_DIRECTORY) if isfile(join(DATA_DIRECTORY, f)) and f.split('.')[1] == 'ann']\n",
    "\n",
    "for file in ann_data_files:\n",
    "    entities = []\n",
    "\n",
    "    # process .ann file - place entities and relations into 2 seperate lists of tuples\n",
    "    with open(join(DATA_DIRECTORY, file), 'r') as document_anno_file:\n",
    "        lines = document_anno_file.readlines()\n",
    "        for line in lines:\n",
    "            standoff_line = line.split()\n",
    "            entity = {}\n",
    "            entity['standoff_id'] = int(standoff_line[0][1:])\n",
    "            entity['entity_type'] = standoff_line[1].capitalize()\n",
    "            entity['offset_start'] = int(standoff_line[2])\n",
    "            entity['offset_end'] = int(standoff_line[3])\n",
    "            entity['word'] = standoff_line[4]\n",
    "            entities.append(entity)\n",
    "\n",
    "    # read the .ann's matching .txt file and tokenize its text using stanford corenlp\n",
    "    with open(join(DATA_DIRECTORY, file.replace('.ann', '.txt')), 'r') as document_text_file:\n",
    "        document_text = document_text_file.read()\n",
    "\n",
    "    output = annotate_posting(document_text)\n",
    "\n",
    "    # write text and annotations into NER\n",
    "    with open(NER_TRAINING_DATA_OUTPUT_PATH, 'a') as ner_training_data:\n",
    "        for sentence in output['sentences']:\n",
    "            entities_in_sentence = {}\n",
    "            sentence_re_rows = []\n",
    "\n",
    "            for token in sentence['tokens']:\n",
    "                offset_start = int(token['characterOffsetBegin'])\n",
    "                offset_end = int(token['characterOffsetEnd'])\n",
    "\n",
    "                re_row = {}\n",
    "                entity_found = False\n",
    "                ner_anno = DEFAULT_OTHER_ANNO\n",
    "\n",
    "                # searching for token in annotated entities\n",
    "                for entity in entities:\n",
    "                    if offset_start >= entity['offset_start'] and offset_end <= entity['offset_end']:\n",
    "                        ner_anno = entity['entity_type']\n",
    "\n",
    "                    # multi-token entities for RE need to be handled differently than NER\n",
    "                    if offset_start == entity['offset_start'] and offset_end <= entity['offset_end']:\n",
    "                        entities_in_sentence[entity['standoff_id']] = len(sentence_re_rows)\n",
    "                        re_row['entity_type'] = entity['entity_type']\n",
    "                        re_row['pos_tag'] = token['pos']\n",
    "                        re_row['word'] = token['word']\n",
    "\n",
    "                        sentence_re_rows.append(re_row)\n",
    "                        entity_found = True\n",
    "                        break\n",
    "                    elif offset_start > entity['offset_start'] and offset_end <= entity['offset_end'] and len(\n",
    "                            sentence_re_rows) > 0:\n",
    "                        sentence_re_rows[-1]['pos_tag'] += '/{}'.format(token['pos'])\n",
    "                        sentence_re_rows[-1]['word'] += '/{}'.format(token['word'])\n",
    "                        entity_found = True\n",
    "                        break\n",
    "\n",
    "                if not entity_found:\n",
    "                    re_row['entity_type'] = DEFAULT_OTHER_ANNO\n",
    "                    re_row['pos_tag'] = token['pos']\n",
    "                    re_row['word'] = token['word']\n",
    "\n",
    "                    sentence_re_rows.append(re_row)\n",
    "\n",
    "                # writing tagged tokens to NER training data\n",
    "                ner_training_data.write('{}\\t{}\\n'.format(token['word'], ner_anno))\n",
    "\n",
    "            sentence_count += 1\n",
    "\n",
    "        ner_training_data.write('\\n')\n",
    "\n",
    "    print('Processed file pair: {} and {}'.format(file, file.replace('.ann', '.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DON'T RUN YET, THIS TAKES A TEXT FILE, RUNS A SPECIFIED NER MODEL ON IT, AND OUTPUTS TO A SPECIFIED OUTPUT DIRECTORY\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "commands = '''\n",
    "cd /Users/nazih.bissat/Desktop/match.fracking/stanford-ner-tagger;\n",
    "java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier conll-train-ner-model.ser.gz -outputFormat inlineXML -textFile untitled.txt > test_file.tsv\n",
    "'''\n",
    "\n",
    "process = Popen('/bin/bash', stdin=PIPE, stdout=PIPE)\n",
    "out, err = process.communicate(commands.encode('utf-8'))\n",
    "\n",
    "full_entities = out.decode('utf-8')\n",
    "print(out.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<I-ORG>HUB</I-ORG> Order <I-PER>Puller</I-PER>.\n",
      "HUB Order Puller.\n",
      "[(0, 7), (10, 18), (25, 32), (38, 46)]\n"
     ]
    }
   ],
   "source": [
    "## TEST TO CONVERT OUTPUT OF STANFORD NLP NER MODEL TO BRAT STANDOFF FORMAT (1)\n",
    "test = '<I-ORG>HUB</I-ORG> Order <I-PER>Puller</I-PER>.'\n",
    "sent = re.sub(r'\\<(.*?)>', '', test)\n",
    "print(test)\n",
    "print(sent)\n",
    "\n",
    "regex = re.compile(r'\\<(.*?)>')\n",
    "iterator = regex.finditer(test)\n",
    "\n",
    "ind = list()\n",
    "for i in iterator:\n",
    "    ind.append(i.span())\n",
    "\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: HUB\n",
      "startChar: 0\n",
      "endChar: 3\n",
      "\n",
      "Entity: Puller\n",
      "startChar: 10\n",
      "endChar: 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TEST TO CONVERT OUTPUT OF STANFORD NLP NER MODEL TO BRAT STANDOFF FORMAT (2)\n",
    "import numpy as np\n",
    "\n",
    "test = '<I-ORG>HUB</I-ORG> Order <I-PER>Puller</I-PER>.'\n",
    "sent = re.sub(r'\\<(.*?)>', '', test)\n",
    "\n",
    "regex = re.compile(r'\\<(.*?)>')\n",
    "iterator = regex.finditer(test)\n",
    "\n",
    "ind = list()\n",
    "for i in iterator:\n",
    "    ind.append(i.span())\n",
    "    \n",
    "counter = 0\n",
    "for i in np.arange(0, len(ind), 2):\n",
    "    print('Entity:', sent[(ind[i][0] - counter):(ind[i+1][0] + ind[i][0] - ind[i][1] - counter)])\n",
    "    print('startChar:', (ind[i][0] - counter))\n",
    "    print('endChar:', ind[i+1][0] + ind[i][0] - ind[i][1] - counter)\n",
    "    print()\n",
    "    counter += 2 * (ind[i][1] - ind[i][0]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
