{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake\n",
    "import getpass\n",
    "import pprint\n",
    "from snowflake.connector import DictCursor\n",
    "from preprocessing import *\n",
    "from postings_ner import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from subprocess import Popen, PIPE\n",
    "import argparse\n",
    "\n",
    "CONNECTION_PARAMS = {\n",
    "    \"user\":None,\n",
    "    \"password\":None,\n",
    "    \"account\":'snagajob',\n",
    "    \"authenticator\":\"https://snagajob.okta.com/\",\n",
    "    \"database\":\"PROD_SAJ_SHARE\",\n",
    "    \"warehouse\":\"PROD_WH\",\n",
    "    \"schema\":\"CUSTOMER\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONNECTION_PARAMS[\"user\"] = input(\"your snowflake user: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONNECTION_PARAMS[\"password\"] = getpass.getpass(\"your snowflake password: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO USE READY SET OOF POSTINGS - USING FIXED SET FOR NOW\n",
    "def load_postings():\n",
    "    with open('1000-postings.pkl', 'rb') as pkl_file:\n",
    "        results = pickle.load(pkl_file)\n",
    "    return results\n",
    "\n",
    "results = load_postings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TAKES A DATE (YYYY-MM-DD)\n",
    "def get_snowflake_data(date_from):\n",
    "    query_p1 = \"\"\"select JOBDESCRIPTION from CUSTOMER.DIMJOBPOSTING_VIEW where CREATEDATE >= \"\"\"\n",
    "    query_p2 = '::date limit 100;'\n",
    "    with snowflake.connector.connect(** CONNECTION_PARAMS) as ctx:\n",
    "        with ctx.cursor(DictCursor) as cs:\n",
    "            with cs.execute(query) as results:\n",
    "                results = [r for r in results]\n",
    "            \n",
    "    with mp.Pool(mp.cpu_count(), worker_init) as pool:\n",
    "        try:\n",
    "            results = pool.map(scrub_posting, results)\n",
    "        except KeyboardInterrupt:\n",
    "            pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DUMP SET OF POSTINGS FOR FUTURE USE\n",
    "def dump_postings():\n",
    "    with open('postings-6-29.pkl', 'wb') as output:\n",
    "        pickle.dump(results, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stfd_entity_types = {'ORGANIZATION': 'Organization', 'LOCATION': 'Location', 'PERSON': 'Person', 'MONEY': 'Money', \n",
    "                     'TIME': 'Time', 'DURATION': 'Duration', 'NUMBER': 'Number', 'PERCENT': 'Percent', \n",
    "                     'MISC': 'Miscellaneous', 'DATE': 'Date', 'ORDINAL': 'Ordinal'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING POSTING TEXT FILES AND RESPECTIVE ENTITY ANNOTATION FILES FOR BRAT TEXT ANNOTATION TOOL (StanfordNLP)\n",
    "\n",
    "def insertPeriod(position, mystring):\n",
    "    longi = len(mystring)\n",
    "    mystring   =  mystring[:position] + '.' + mystring[position:] \n",
    "    return mystring\n",
    "\n",
    "def postings_to_brat(data_dir):\n",
    "    # StanfordNLP NER\n",
    "    startup_corenlp_server()\n",
    "    \n",
    "    posting_index = 1\n",
    "    for r in results:\n",
    "        posting_fname = 'posting' + str(posting_index)\n",
    "        posting_text = r['JD_SCRUBBED'].strip()\n",
    "        posting_text = re.sub(r'(\\n-)', '\\n', posting_text)\n",
    "        posting_text = re.sub(r'(\\n  -)', '\\n', posting_text)\n",
    "        posting_text = re.sub(r'(\\n  -)', '\\n', posting_text)\n",
    "        posting_text = re.sub(r'(\\n  )', '\\n', posting_text)\n",
    "        \n",
    "        indices = [x.start() for x in re.finditer(r'\\n', posting_text)]\n",
    "\n",
    "        counter = 0\n",
    "        for i in indices:\n",
    "            if posting_text[i+counter-1] != '.':\n",
    "                if posting_text[i+counter-1] != '\\n':\n",
    "                    posting_text = insertPeriod(i+counter, posting_text)\n",
    "                    counter += 1\n",
    "    \n",
    "#         with open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/Training_NER/StanfordNLP/' + posting_fname + '.txt', 'w') as text_file:\n",
    "#             text_file.write(posting_text)\n",
    "#             text_file.close()\n",
    "        with open(data_dir + posting_fname + '.txt', 'w') as text_file:\n",
    "            text_file.write(posting_text)\n",
    "            text_file.close()\n",
    "    \n",
    "        entity_index = 1\n",
    "        entity_ann_file_text = ''\n",
    "        posting_details = annotate_posting(posting_text)\n",
    "        for s in posting_details['sentences']:\n",
    "            for e in s['entitymentions']:\n",
    "                entity_ann_file_text += 'T' + str(entity_index) + '\\t' + stfd_entity_types[e['ner']] + ' ' \\\n",
    "                                        + str(e['characterOffsetBegin']) + ' ' + str(e['characterOffsetEnd']) + '\\t' \\\n",
    "                                        + e['text'] + '\\n'\n",
    "                entity_index += 1\n",
    "        \n",
    "#         ann_file = open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/Training_NER/StanfordNLP/' + posting_fname + '.ann', 'w')\n",
    "        ann_file = open(data_dir + posting_fname + '.ann', 'w')\n",
    "        ann_file.write(entity_ann_file_text)\n",
    "        ann_file.close()\n",
    "        \n",
    "        posting_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = load_postings()\n",
    "postings_to_brat('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/Training_NER/StanfordNLP/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file pair: posting16.ann and posting16.txt\n",
      "Processed file pair: posting17.ann and posting17.txt\n",
      "Processed file pair: posting29.ann and posting29.txt\n",
      "Processed file pair: posting15.ann and posting15.txt\n",
      "Processed file pair: posting100.ann and posting100.txt\n",
      "Processed file pair: posting14.ann and posting14.txt\n",
      "Processed file pair: posting28.ann and posting28.txt\n",
      "Processed file pair: posting10.ann and posting10.txt\n",
      "Processed file pair: posting38.ann and posting38.txt\n",
      "Processed file pair: posting39.ann and posting39.txt\n",
      "Processed file pair: posting11.ann and posting11.txt\n",
      "Processed file pair: posting13.ann and posting13.txt\n",
      "Processed file pair: posting12.ann and posting12.txt\n",
      "Processed file pair: posting49.ann and posting49.txt\n",
      "Processed file pair: posting61.ann and posting61.txt\n",
      "Processed file pair: posting75.ann and posting75.txt\n",
      "Processed file pair: posting74.ann and posting74.txt\n",
      "Processed file pair: posting60.ann and posting60.txt\n",
      "Processed file pair: posting48.ann and posting48.txt\n",
      "Processed file pair: posting89.ann and posting89.txt\n",
      "Processed file pair: posting76.ann and posting76.txt\n",
      "Processed file pair: posting62.ann and posting62.txt\n",
      "Processed file pair: posting63.ann and posting63.txt\n",
      "Processed file pair: posting77.ann and posting77.txt\n",
      "Processed file pair: posting88.ann and posting88.txt\n",
      "Processed file pair: posting98.ann and posting98.txt\n",
      "Processed file pair: posting73.ann and posting73.txt\n",
      "Processed file pair: posting67.ann and posting67.txt\n",
      "Processed file pair: posting9.ann and posting9.txt\n",
      "Processed file pair: posting8.ann and posting8.txt\n",
      "Processed file pair: posting66.ann and posting66.txt\n",
      "Processed file pair: posting72.ann and posting72.txt\n",
      "Processed file pair: posting99.ann and posting99.txt\n",
      "Processed file pair: posting64.ann and posting64.txt\n",
      "Processed file pair: posting70.ann and posting70.txt\n",
      "Processed file pair: posting58.ann and posting58.txt\n",
      "Processed file pair: posting59.ann and posting59.txt\n",
      "Processed file pair: posting71.ann and posting71.txt\n",
      "Processed file pair: posting65.ann and posting65.txt\n",
      "Processed file pair: posting83.ann and posting83.txt\n",
      "Processed file pair: posting97.ann and posting97.txt\n",
      "Processed file pair: posting68.ann and posting68.txt\n",
      "Processed file pair: posting40.ann and posting40.txt\n",
      "Processed file pair: posting54.ann and posting54.txt\n",
      "Processed file pair: posting6.ann and posting6.txt\n",
      "Processed file pair: posting7.ann and posting7.txt\n",
      "Processed file pair: posting55.ann and posting55.txt\n",
      "Processed file pair: posting41.ann and posting41.txt\n",
      "Processed file pair: posting69.ann and posting69.txt\n",
      "Processed file pair: posting96.ann and posting96.txt\n",
      "Processed file pair: posting82.ann and posting82.txt\n",
      "Processed file pair: posting94.ann and posting94.txt\n",
      "Processed file pair: posting80.ann and posting80.txt\n",
      "Processed file pair: posting57.ann and posting57.txt\n",
      "Processed file pair: posting43.ann and posting43.txt\n",
      "Processed file pair: posting5.ann and posting5.txt\n",
      "Processed file pair: posting4.ann and posting4.txt\n",
      "Processed file pair: posting42.ann and posting42.txt\n",
      "Processed file pair: posting56.ann and posting56.txt\n",
      "Processed file pair: posting81.ann and posting81.txt\n",
      "Processed file pair: posting95.ann and posting95.txt\n",
      "Processed file pair: posting91.ann and posting91.txt\n",
      "Processed file pair: posting85.ann and posting85.txt\n",
      "Processed file pair: posting52.ann and posting52.txt\n",
      "Processed file pair: posting46.ann and posting46.txt\n",
      "Processed file pair: posting1.ann and posting1.txt\n",
      "Processed file pair: posting47.ann and posting47.txt\n",
      "Processed file pair: posting53.ann and posting53.txt\n",
      "Processed file pair: posting84.ann and posting84.txt\n",
      "Processed file pair: posting90.ann and posting90.txt\n",
      "Processed file pair: posting86.ann and posting86.txt\n",
      "Processed file pair: posting92.ann and posting92.txt\n",
      "Processed file pair: posting45.ann and posting45.txt\n",
      "Processed file pair: posting51.ann and posting51.txt\n",
      "Processed file pair: posting79.ann and posting79.txt\n",
      "Processed file pair: posting3.ann and posting3.txt\n",
      "Processed file pair: posting2.ann and posting2.txt\n",
      "Processed file pair: posting78.ann and posting78.txt\n",
      "Processed file pair: posting50.ann and posting50.txt\n",
      "Processed file pair: posting44.ann and posting44.txt\n",
      "Processed file pair: posting93.ann and posting93.txt\n",
      "Processed file pair: posting87.ann and posting87.txt\n",
      "Processed file pair: posting23.ann and posting23.txt\n",
      "Processed file pair: posting37.ann and posting37.txt\n",
      "Processed file pair: posting36.ann and posting36.txt\n",
      "Processed file pair: posting22.ann and posting22.txt\n",
      "Processed file pair: posting34.ann and posting34.txt\n",
      "Processed file pair: posting20.ann and posting20.txt\n",
      "Processed file pair: posting21.ann and posting21.txt\n",
      "Processed file pair: posting35.ann and posting35.txt\n",
      "Processed file pair: posting31.ann and posting31.txt\n",
      "Processed file pair: posting25.ann and posting25.txt\n",
      "Processed file pair: posting19.ann and posting19.txt\n",
      "Processed file pair: posting18.ann and posting18.txt\n",
      "Processed file pair: posting24.ann and posting24.txt\n",
      "Processed file pair: posting30.ann and posting30.txt\n",
      "Processed file pair: posting26.ann and posting26.txt\n",
      "Processed file pair: posting32.ann and posting32.txt\n",
      "Processed file pair: posting33.ann and posting33.txt\n",
      "Processed file pair: posting27.ann and posting27.txt\n"
     ]
    }
   ],
   "source": [
    "# A python script to turn annotated data in standoff format (brat annotation tool) to the formats expected by Stanford NER and Relation Extractor models\n",
    "# - NER format based on: http://nlp.stanford.edu/software/crf-faq.html#a\n",
    "\n",
    "def compile_training_data(data_dir):\n",
    "    DEFAULT_OTHER_ANNO = 'O'\n",
    "# #     IF RUNNING ON MY MACHINE, USE THE PATH BELOW\n",
    "#     DATA_DIRECTORY = '/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/Training_NER/StanfordNLP'\n",
    "    DATA_DIRECTORY = data_dir\n",
    "    OUTPUT_DIRECTORY = 'stanford-nlp-train'\n",
    "    \n",
    "    NER_TRAINING_DATA_OUTPUT_PATH = join(OUTPUT_DIRECTORY, 'stanford-nlp-training-data.tsv')\n",
    "    \n",
    "    if os.path.exists(OUTPUT_DIRECTORY):\n",
    "        if os.path.exists(NER_TRAINING_DATA_OUTPUT_PATH):\n",
    "            os.remove(NER_TRAINING_DATA_OUTPUT_PATH)\n",
    "    else:\n",
    "        os.makedirs(OUTPUT_DIRECTORY)\n",
    "    \n",
    "    sentence_count = 0\n",
    "#     startup_corenlp_server()\n",
    "\n",
    "    # looping through .ann files in the data directory\n",
    "    ann_data_files = [f for f in listdir(DATA_DIRECTORY) if isfile(join(DATA_DIRECTORY, f)) and f.split('.')[1] == 'ann']\n",
    "\n",
    "    for file in ann_data_files:\n",
    "        entities = []\n",
    "    \n",
    "        # process .ann file - place entities and relations into 2 seperate lists of tuples\n",
    "        with open(join(DATA_DIRECTORY, file), 'r') as document_anno_file:\n",
    "            lines = document_anno_file.readlines()\n",
    "            for line in lines:\n",
    "                standoff_line = line.split()\n",
    "                entity = {}\n",
    "                entity['standoff_id'] = int(standoff_line[0][1:])\n",
    "                entity['entity_type'] = standoff_line[1].capitalize()\n",
    "                entity['offset_start'] = int(standoff_line[2])\n",
    "                entity['offset_end'] = int(standoff_line[3])\n",
    "                entity['word'] = standoff_line[4]\n",
    "                entities.append(entity)\n",
    "    \n",
    "        # read the .ann's matching .txt file and tokenize its text using stanford corenlp\n",
    "        with open(join(DATA_DIRECTORY, file.replace('.ann', '.txt')), 'r') as document_text_file:\n",
    "            document_text = document_text_file.read()\n",
    "    \n",
    "        output = annotate_posting(document_text)\n",
    "    \n",
    "        # write text and annotations into NER\n",
    "        with open(NER_TRAINING_DATA_OUTPUT_PATH, 'a') as ner_training_data:\n",
    "            for sentence in output['sentences']:\n",
    "                entities_in_sentence = {}\n",
    "                sentence_re_rows = []\n",
    "    \n",
    "                for token in sentence['tokens']:\n",
    "                    offset_start = int(token['characterOffsetBegin'])\n",
    "                    offset_end = int(token['characterOffsetEnd'])\n",
    "                    \n",
    "                    re_row = {}\n",
    "                    entity_found = False\n",
    "                    ner_anno = DEFAULT_OTHER_ANNO\n",
    "\n",
    "                    # searching for token in annotated entities\n",
    "                    for entity in entities:\n",
    "                        if offset_start >= entity['offset_start'] and offset_end <= entity['offset_end']:\n",
    "                            ner_anno = entity['entity_type']\n",
    "                        \n",
    "                        # multi-token entities for RE need to be handled differently than NER\n",
    "                        if offset_start == entity['offset_start'] and offset_end <= entity['offset_end']:\n",
    "                            entities_in_sentence[entity['standoff_id']] = len(sentence_re_rows)\n",
    "                            re_row['entity_type'] = entity['entity_type']\n",
    "                            re_row['pos_tag'] = token['pos']\n",
    "                            re_row['word'] = token['word']\n",
    "                        \n",
    "                            sentence_re_rows.append(re_row)\n",
    "                            entity_found = True\n",
    "                            break\n",
    "                        elif offset_start > entity['offset_start'] and offset_end <= entity['offset_end'] and len(\n",
    "                                sentence_re_rows) > 0:\n",
    "                            sentence_re_rows[-1]['pos_tag'] += '/{}'.format(token['pos'])\n",
    "                            sentence_re_rows[-1]['word'] += '/{}'.format(token['word'])\n",
    "                            entity_found = True\n",
    "                            break\n",
    "                        \n",
    "                    if not entity_found:\n",
    "                        re_row['entity_type'] = DEFAULT_OTHER_ANNO\n",
    "                        re_row['pos_tag'] = token['pos']\n",
    "                        re_row['word'] = token['word']\n",
    "                        \n",
    "                        sentence_re_rows.append(re_row)\n",
    "\n",
    "                    # writing tagged tokens to NER training data\n",
    "                    ner_training_data.write('{}\\t{}\\n'.format(token['word'], ner_anno))\n",
    "\n",
    "                sentence_count += 1\n",
    "\n",
    "            ner_training_data.write('\\n')\n",
    "\n",
    "        print('Processed file pair: {} and {}'.format(file, file.replace('.ann', '.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### COMMAND TO TRAIN STANFORDNLP NER: java -cp \"stanford-ner.jar:lib/*\" -mx4g edu.stanford.nlp.ie.crf.CRFClassifier -prop train/prop.txt\n",
    "\n",
    "def train_ner_model(stfd_ner_dir):\n",
    "    ## CHANGE THIS\n",
    "#     commands = '''\n",
    "#     cd /Users/nazih.bissat/Desktop/match.fracking/stanford-ner-tagger;\n",
    "#     java -cp \"stanford-ner.jar:lib/*\" -mx4g edu.stanford.nlp.ie.crf.CRFClassifier -prop train_stanford_nlp/prop.txt\n",
    "#     '''\n",
    "    commands = 'cd '\n",
    "    commands += stfd_ner_dir + ';'\n",
    "    commands += '''\n",
    "    java -cp \"stanford-ner.jar:lib/*\" -mx4g edu.stanford.nlp.ie.crf.CRFClassifier -prop train_stanford_nlp/prop.txt\n",
    "    '''\n",
    "    \n",
    "    process = Popen('/bin/bash', stdin=PIPE, stdout=PIPE)\n",
    "    out, err = process.communicate(commands.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS TAKES A TEXT FILE, RUNS A SPECIFIED NER MODEL ON IT, AND OUTPUTS TO A SPECIFIED OUTPUT DIRECTORY\n",
    "\n",
    "def posting_ner(stfd_ner_dir, data_dir, f):\n",
    "#     CHANGE THIS\n",
    "#     commands = '''\n",
    "#     cd /Users/nazih.bissat/Desktop/match.fracking/stanford-ner-tagger;\n",
    "#     java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier postings-ner-model.ser.gz -outputFormat inlineXML -textFile '''\n",
    "    commands = 'cd '\n",
    "    commands += stfd_ner_dir + ';'\n",
    "    commands += '''\n",
    "    java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier postings-ner-model.ser.gz -outputFormat inlineXML -textFile '''\n",
    "# \n",
    "    commands += data_dir + '/' + f + '.txt > ' + f + '-ner.txt'\n",
    "\n",
    "    process = Popen('/bin/bash', stdin=PIPE, stdout=PIPE)\n",
    "    out, err = process.communicate(commands.encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posting_ner('/Users/nazih.bissat/Desktop/match.fracking/stanford-ner-tagger/', '/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/Training_NER/StanfordNLP', 'posting1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING POSTING TEXT FILES AND RESPECTIVE ENTITY ANNOTATION FILES FOR BRAT TEXT ANNOTATION TOOL (StanfordNLP)\n",
    "\n",
    "def reannotate_postings(data_dir, stfd_ner_dir):\n",
    "#     DATA_DIRECTORY = '/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/Training_NER/StanfordNLP'\n",
    "    DATA_DIRECTORY = data_dir\n",
    "    text_files = [f for f in listdir(DATA_DIRECTORY) if isfile(join(DATA_DIRECTORY, f)) and f.split('.')[1] == 'txt']\n",
    "    \n",
    "    for f in text_files:\n",
    "        file_path = join(DATA_DIRECTORY, f)\n",
    "        posting_fname = f.split('.')[0]\n",
    "        \n",
    "        with open(file_path, 'r') as posting_file:\n",
    "            posting_text = posting_file.read()\n",
    "    \n",
    "        entity_index = 1\n",
    "        entity_ann_file_text = ''\n",
    "        posting_details = posting_ner(stfd_ner_dir, DATA_DIRECTORY, posting_fname)\n",
    "#         with open('/Users/nazih.bissat/Desktop/match.fracking/stanford-ner-tagger/' + posting_fname + '-ner.txt', 'r') as o:\n",
    "#             ner_output = o.read()\n",
    "        with open(stfd_ner_dir + posting_fname + '-ner.txt', 'r') as o:\n",
    "            ner_output = o.read()    \n",
    "        \n",
    "        regex = re.compile(r'\\<(.*?)>')\n",
    "        iterator = regex.finditer(ner_output)\n",
    "        \n",
    "        ind = list()\n",
    "        for i in iterator:\n",
    "            ind.append(i.span())\n",
    "    \n",
    "        counter = 0\n",
    "        for i in np.arange(0, len(ind), 2):\n",
    "            entity_text = posting_text[(ind[i][0] - counter):(ind[i+1][0] + ind[i][0] - ind[i][1] - counter)]\n",
    "            entity_type = ner_output[(ind[i][0]+1):(ind[i][1]-1)]\n",
    "            start_char = ind[i][0] - counter\n",
    "            end_char = ind[i+1][0] + ind[i][0] - ind[i][1] - counter\n",
    "            entity_ann_file_text += 'T' + str(entity_index) + '\\t' + entity_type + ' ' \\\n",
    "                                        + str(start_char) + ' ' + str(end_char) + '\\t' \\\n",
    "                                        + entity_text + '\\n'\n",
    "            entity_index += 1\n",
    "            counter += 2 * (ind[i][1] - ind[i][0]) + 1\n",
    "    \n",
    "        with open(join(DATA_DIRECTORY, posting_fname + '.ann'), 'w') as ann_file:\n",
    "            ann_file.write(entity_ann_file_text)\n",
    "            ann_file.close()\n",
    "        \n",
    "        os.remove(stfd_ner_dir + posting_fname + '-ner.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reannotate_postings('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/Training_NER/StanfordNLP','/Users/nazih.bissat/Desktop/match.fracking/stanford-ner-tagger/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutdown_corenlp_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--retrain RETRAIN] [--data_dir DATA_DIR]\n",
      "                             [--stfd_ner_dir STFD_NER_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/nazih.bissat/Library/Jupyter/runtime/kernel-3ebf6e78-0c6a-4a87-8450-a01f5e5af60d.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nazih.bissat/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(retrain, data_dir, stfd_ner_dir):\n",
    "    if retrain == False:\n",
    "        load_postings()\n",
    "        startup_corenlp_server()\n",
    "        postings_to_brat(data_dir)\n",
    "        compile_training_data(data_dir)\n",
    "        train_ner_model(stfd_ner_dir) ## ADD ARGS\n",
    "        reannotate_postings(data_dir, stfd_ner_dir)\n",
    "        shutdown_corenlp_server()\n",
    "    else:\n",
    "        startup_corenlp_server()\n",
    "#         train_ner_model(stfd_ner_dir)\n",
    "        reannotate_postings(data_dir, stfd_ner_dir)\n",
    "        shutdown_corenlp_server()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ## ADD ARGS\n",
    "    parser = argparse.ArgumentParser(description = 'Input the path to the correct folder in the brat data directory and the path to the stanford-ner-tagger directory.')\n",
    "    parser.add_argument('retrain', type=bool, help='Specify whether the call is intended to retrain an existing model or train a blank model.')\n",
    "    parser.add_argument('data_dir', help='Path to the correct folder in the brat data directory')\n",
    "    parser.add_argument('stfd_ner_dir', help='Path to the stanford-ner-tagger directory.')\n",
    "    args = parser.parse_args()\n",
    "    main(args.retrain, args.data_dir, args.stfd_ner_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
