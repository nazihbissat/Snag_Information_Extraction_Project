{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER and Information Extraction POC/Research\n",
    "\n",
    "1. walk through IE tutorials and examine results on real postings data\n",
    "1. define priorities and use cases \n",
    "    1. current systems (look at fextures definitions, wage data extraction, talk to John Moon)\n",
    "        1. https://github.com/Snagajob/match.fexter-rules/blob/master/features.cfg\n",
    "        1. https://github.com/Snagajob/match.wage\n",
    "    1. use cases:\n",
    "        1. brands \n",
    "        1. job titles \n",
    "        1. wages\n",
    "        1. shift information\n",
    "        1. location information\n",
    "        1. requirements\n",
    "        1. ...\n",
    "1. research, build list and evaluate open source IE tools/models and potentially useful corpora\n",
    "    1. NLTK (research only)\n",
    "    1. Stanford NLP toolkit (https://nlp.stanford.edu/software/openie.html)\n",
    "    1. Spacy (https://spacy.io/)\n",
    "    1. Open NLP (https://opennlp.apache.org/)\n",
    "    1. Resources list: https://nlp.stanford.edu/links/statnlp.html#NER\n",
    "    1. ???\n",
    "1. annotation tool:\n",
    "    1. http://brat.nlplab.org/\n",
    "\n",
    "## Where to Start\n",
    "\n",
    "- useful scripts and lexicons to adapt:\n",
    "    - https://github.com/robbymeals/alto-boot/blob/master/scripts/generate_text_data.py\n",
    "    - https://github.com/robbymeals/alto-boot/tree/master/nlp_resources\n",
    "    - don't pull from mongo directly, use snowflake table: CUSTOMER.DIMJOBPOSTING_VIEW\n",
    "- week of June 11: walk through tutorials and documentation, begin creating list of various tools\n",
    "- week of June 18: complete and review list of tools, start working on comparison demo notebook\n",
    "- week of June 25: complete and review comparison notebook, plan next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake\n",
    "import getpass\n",
    "import pprint\n",
    "from snowflake.connector import DictCursor\n",
    "from preprocessing import *\n",
    "from postings_ner import *\n",
    "\n",
    "CONNECTION_PARAMS = {\n",
    "    \"user\":None,\n",
    "    \"password\":None,\n",
    "    \"account\":'snagajob',\n",
    "    \"authenticator\":\"https://snagajob.okta.com/\",\n",
    "    \"database\":\"PROD_SAJ_SHARE\",\n",
    "    \"warehouse\":\"PROD_WH\",\n",
    "    \"schema\":\"CUSTOMER\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your snowflake user: nazih.bissat@snagajob.com\n"
     ]
    }
   ],
   "source": [
    "CONNECTION_PARAMS[\"user\"] = input(\"your snowflake user: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your snowflake password: ········\n"
     ]
    }
   ],
   "source": [
    "CONNECTION_PARAMS[\"password\"] = getpass.getpass(\"your snowflake password: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select JOBDESCRIPTION from CUSTOMER.DIMJOBPOSTING_VIEW where CREATEDATE >= '2018-07-01'::date limit 1000;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with snowflake.connector.connect(** CONNECTION_PARAMS) as ctx:\n",
    "    with ctx.cursor(DictCursor) as cs:\n",
    "        with cs.execute(query) as results:\n",
    "            results = [r for r in results] # you can leave as generator or feed directly in pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(mp.cpu_count(), worker_init) as pool:\n",
    "    try:\n",
    "        results = pool.map(scrub_posting, results)\n",
    "    except KeyboardInterrupt:\n",
    "        pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "output = open('1000-postings.pkl', 'wb')\n",
    "pickle.dump(results, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_entity_types = {'ORG': 'Organization', 'PRODUCT': 'Product', 'CARDINAL': 'Cardinal', 'PERSON': 'Person',\n",
    "                     'TIME': 'Time', 'DATE': 'Date', 'PERCENT': 'Percent', 'GPE': 'Location_Geo-Political_Area',\n",
    "                     'WORK_OF_ART': 'Job_Title_or_Degree', 'LANGUAGE': 'Language', 'QUANTITY': 'Quantity',\n",
    "                     'EVENT': 'Event', 'MONEY': 'Money', 'NORP': 'Nationality_or_Religion_or_Political_group',\n",
    "                     'ORDINAL': 'Ordinal', 'FAC': 'Facility', 'LOC': 'Location', 'LAW': 'Law'}\n",
    "\n",
    "stfd_entity_types = {'ORGANIZATION': 'Organization', 'TITLE': 'Title', 'LOCATION': 'Location', 'EMAIL': 'Email',\n",
    "               'URL': 'URL', 'CITY': 'Location_City', 'STATE_OR_PROVINCE': 'Location_State_or_Province',\n",
    "               'COUNTRY': 'Location_Country', 'NATIONALITY': 'Nationality', 'RELIGION': 'Religion', 'TITLE': 'Title',\n",
    "               'IDEOLOGY': 'Ideology', 'CRIMINAL_CHARGE': 'Criminal_Charge', 'CAUSE_OF_DEATH': 'Cause_of_Death',\n",
    "               'PERSON': 'Person', 'MONEY': 'Money', 'TIME': 'Time', 'ORDINAL': 'Ordinal', 'SET': 'Set',\n",
    "               'DURATION': 'Duration', 'NUMBER': 'Number', 'PERCENT': 'Percent', 'MISC': 'Miscellaneous',\n",
    "                'DATE': 'Date'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING POSTING TEXT FILES AND RESPECTIVE ENTITY ANNOTATION FILES FOR BRAT TEXT ANNOTATION TOOL (SPACY)\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "posting_index = 1\n",
    "for r in results:\n",
    "    posting_fname = 'posting' + str(posting_index)\n",
    "    posting_text = r['JD_SCRUBBED']\n",
    "    \n",
    "    with open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/NER/spaCy/' + posting_fname + '.txt', 'w') as text_file:\n",
    "        text_file.write(posting_text)\n",
    "        text_file.close()\n",
    "    \n",
    "    entity_index = 1\n",
    "    entity_ann_file_text = ''\n",
    "    for ent in nlp(r['JD_SCRUBBED']).ents:\n",
    "        entity_ann_file_text += 'T' + str(entity_index) + '\\t' + spacy_entity_types[ent.label_] + ' ' \\\n",
    "                                    + str(ent.start_char) + ' ' + str(ent.end_char) + '\\t' \\\n",
    "                                    + ent.text + '\\n'\n",
    "        entity_index += 1\n",
    "    \n",
    "    ann_file = open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/NER/spaCy/' + posting_fname + '.ann', 'w')\n",
    "    ann_file.write(entity_ann_file_text)\n",
    "    ann_file.close()\n",
    "    \n",
    "    posting_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n  '"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BRAT IS BUGGY WITH THE 100TH POSTING IN SPACY, FIGURING OUT WHY\n",
    "\n",
    "# doc = nlp(\"u'\" + results[99]['JD_SCRUBBED'] + \"'\")\n",
    "#     # doc = nlp(\"u'\" + text + \"'\")\n",
    "# for ent in doc.ents:\n",
    "#     print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "# print(doc.ents)\n",
    "\n",
    "(results[99]['JD_SCRUBBED'][1267:1270])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Come, a place), (we, pride), (that, hard work), (everyone, more sports), (The Cashier, an essential part), (an essential part, customer service), (customer service, any Academy store), (a fast, friendly, and accurate checkout experience, all customers), (impacts, this important work), (symbol, people), (solving, customers), (solving, use), (use, POS equipment), (documents, safety rules), (furnished, written, oral or diagram form\n",
      "  - Working knowledge), (written, oral or diagram form\n",
      "  - Working knowledge, inventory software), (cash register, all transaction types), (all transaction types, returns), (run, an assigned register), (stocks, racks), (knowledge, cashier processes), (- Reads, company work hours), (- Specific vision abilities, close vision), (flexible schedule, nights), (based, business needs)]\n",
      "Come      \t\ta place\n",
      "we        \t\tpride\n",
      "that      \t\thard work\n",
      "everyone  \t\tmore sports\n",
      "The Cashier\t\tan essential part\n",
      "an essential part\t\tcustomer service\n",
      "customer service\t\tany Academy store\n",
      "a fast, friendly, and accurate checkout experience\t\tall customers\n",
      "impacts   \t\tthis important work\n",
      "symbol    \t\tpeople\n",
      "solving   \t\tcustomers\n",
      "solving   \t\tuse\n",
      "use       \t\tPOS equipment\n",
      "documents \t\tsafety rules\n",
      "furnished \t\twritten, oral or diagram form\n",
      "  - Working knowledge\n",
      "written, oral or diagram form\n",
      "  - Working knowledge\t\tinventory software\n",
      "cash register\t\tall transaction types\n",
      "all transaction types\t\treturns\n",
      "run       \t\tan assigned register\n",
      "stocks    \t\tracks\n",
      "knowledge \t\tcashier processes\n",
      "- Reads   \t\tcompany work hours\n",
      "- Specific vision abilities\t\tclose vision\n",
      "flexible schedule\tTIME\tnights\n",
      "based     \t\tbusiness needs\n"
     ]
    }
   ],
   "source": [
    "# Information extraction (relations) - SPACY\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import spacy\n",
    "\n",
    "\n",
    "TEXTS = [results[1]['JD_SCRUBBED']]\n",
    "\n",
    "\n",
    "# @plac.annotations(\n",
    "#     model=(\"Model to load (needs parser and NER)\", \"positional\", None, str))\n",
    "\n",
    "def main(model='en_core_web_sm'):\n",
    "    nlp = spacy.load(model)\n",
    "#     print(\"Loaded model '%s'\" % model)\n",
    "#     print(\"Processing %d texts\" % len(TEXTS))\n",
    "\n",
    "    for text in TEXTS:\n",
    "        doc = nlp(text)\n",
    "        relations = extract_entity_relations(doc)\n",
    "        print(relations)\n",
    "        for r1, r2 in relations:\n",
    "            print('{:<10}\\t{}\\t{}'.format(r1.text, r2.ent_type_, r2.text))\n",
    "\n",
    "\n",
    "def extract_entity_relations(doc):\n",
    "    # merge entities and noun chunks into one token\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "\n",
    "    relations = []\n",
    "    for entity in filter(lambda w: w.ent_type_ != 'ORG', doc):\n",
    "        if entity.dep_ in ('attr', 'dobj'):\n",
    "            subject = [w for w in entity.head.lefts if w.dep_ == 'nsubj']\n",
    "            if subject:\n",
    "                subject = subject[0]\n",
    "                relations.append((subject, entity))\n",
    "        elif entity.dep_ == 'pobj' and entity.head.dep_ == 'prep':\n",
    "            relations.append((entity.head.head, entity))\n",
    "    return relations\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     plac.call(main)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StanfordNLP NER + RegexNER\n",
    "startup_corenlp_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING POSTING TEXT FILES AND RESPECTIVE ENTITY ANNOTATION FILES FOR BRAT TEXT ANNOTATION TOOL (StanfordNLP)\n",
    "\n",
    "posting_index = 1\n",
    "for r in results:\n",
    "    posting_fname = 'posting' + str(posting_index)\n",
    "    posting_text = r['JD_SCRUBBED']\n",
    "    \n",
    "    with open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/NER/StanfordNLP/' + posting_fname + '.txt', 'w') as text_file:\n",
    "        text_file.write(posting_text)\n",
    "        text_file.close()\n",
    "    \n",
    "    entity_index = 1\n",
    "    entity_ann_file_text = ''\n",
    "    posting_details = annotate_posting(posting_text)\n",
    "    for s in posting_details['sentences']:\n",
    "        for e in s['entitymentions']:\n",
    "            entity_ann_file_text += 'T' + str(entity_index) + '\\t' + stfd_entity_types[e['ner']] + ' ' \\\n",
    "                                    + str(e['characterOffsetBegin']) + ' ' + str(e['characterOffsetEnd']) + '\\t' \\\n",
    "                                    + e['text'] + '\\n'\n",
    "            entity_index += 1\n",
    "    \n",
    "    ann_file = open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/NER/StanfordNLP/' + posting_fname + '.ann', 'w')\n",
    "    ann_file.write(entity_ann_file_text)\n",
    "    ann_file.close()\n",
    "    \n",
    "    posting_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutdown_corenlp_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:ccg_nlpy.pipeline_config:Models not found. To use pipeline locally, please refer the documentation for downloading models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2096\n",
      "2103\n",
      "564\n",
      "679\n",
      "1551\n",
      "739\n",
      "1045\n",
      "1241\n",
      "564\n",
      "679\n",
      "739\n",
      "1045\n",
      "1241\n",
      "739\n",
      "1045\n",
      "1241\n",
      "1438\n",
      "1711\n",
      "1817\n",
      "1888\n",
      "1551\n",
      "593\n",
      "posting15 Biopharmaceutical / Vaccine\n",
      "posting15 PHYSICAL / MENTAL REQUIREMENTS Ability\n",
      "3602\n",
      "3886\n",
      "4133\n",
      "4178\n",
      "4372\n",
      "4642\n",
      "564\n",
      "679\n",
      "3611\n",
      "1067\n",
      "1183\n",
      "877\n",
      "992\n",
      "4173\n",
      "3611\n",
      "564\n",
      "679\n",
      "1166\n",
      "1494\n",
      "1589\n",
      "2088\n",
      "2367\n",
      "1909\n",
      "2613\n",
      "2866\n",
      "2934\n",
      "2981\n",
      "1013\n",
      "2187\n",
      "2460\n",
      "2674\n",
      "351\n",
      "504\n",
      "976\n",
      "1174\n",
      "1120\n",
      "788\n",
      "1230\n",
      "1157\n",
      "1522\n",
      "1561\n",
      "1002\n",
      "posting34 Medical / Dental / Vision Advancement Opportunity 401K Positive Working Environment Sales Consultant\n",
      "494\n",
      "915\n",
      "757\n",
      "930\n",
      "958\n",
      "1039\n",
      "1422\n",
      "564\n",
      "679\n",
      "posting39 AAP / EEO Statement The Company\n",
      "735\n",
      "1080\n",
      "279\n",
      "508\n",
      "1112\n",
      "posting42 Allied Business Unit ) Quality Assurance\n",
      "1201\n",
      "1448\n",
      "posting42 Lean / Six Sigma Black Belt - Retail Operations\n",
      "2580\n",
      "2605\n",
      "2820\n",
      "3010\n",
      "3171\n",
      "599\n",
      "739\n",
      "1045\n",
      "1241\n",
      "1438\n",
      "1711\n",
      "1817\n",
      "1888\n",
      "3611\n",
      "564\n",
      "679\n",
      "1367\n",
      "1597\n",
      "1188\n",
      "1843\n",
      "1697\n",
      "1946\n",
      "1977\n",
      "2076\n",
      "2096\n",
      "2103\n",
      "1438\n",
      "1711\n",
      "1817\n",
      "1887\n",
      "494\n",
      "1393\n",
      "1393\n",
      "3611\n",
      "posting64 Sears Auto Center If\n",
      "posting64 YhZoUk2cM3Y { # 560\n",
      "posting64 315 # } #\n",
      "posting64 Job Requirements Job Duties / Responsibilities\n",
      "1619\n",
      "1802\n",
      "posting65 the Store Manager and / or Assistant Store Manager\n",
      "posting65 the Store Manager and / or Assistant Store Manager\n",
      "586\n",
      "1049\n",
      "posting65 the Store Manager and / or Assistant Store Manager\n",
      "posting65 1 ) year\n",
      "posting65 2 ) year\n",
      "posting65 INDEEDL Equal Opportunity Employer Minorities / Women / Protected Veterans /\n",
      "1560\n",
      "1746\n",
      "posting66 Equal Opportunity Employer Minorities / Women / Protected Veterans /\n",
      "371\n",
      "posting68 Equal Opportunity Employer Minorities / Women / Protected Veteran / Disabled Equal Opportunity Employer Minorities / Women / Protected Veterans /\n",
      "3611\n",
      "548\n",
      "705\n",
      "1684\n",
      "1957\n",
      "2063\n",
      "2134\n",
      "494\n",
      "718\n",
      "1024\n",
      "1069\n",
      "1013\n",
      "2187\n",
      "2460\n",
      "2674\n",
      "599\n",
      "1057\n",
      "posting78 Equal Opportunity Employer Minorities / Women / Protected Veterans /\n",
      "posting79 Sears Auto Center If\n",
      "posting79 YhZoUk2cM3Y { # 560\n",
      "posting79 315 # } #\n",
      "posting79 Job Requirements Job Duties / Responsibilities\n",
      "1619\n",
      "1802\n",
      "439\n",
      "615\n",
      "894\n",
      "1351\n",
      "posting80 High School Diploma / GED\n",
      "3654\n",
      "3674\n",
      "posting81 Equal Opportunity Employer Minorities / Women / Protected Veterans /\n",
      "819\n",
      "posting82 QSR / ISO\n",
      "posting82 Education / Experience Requirements\n",
      "posting83 Speech / Language Pathology\n",
      "548\n",
      "705\n",
      "1684\n",
      "1957\n",
      "2063\n",
      "2133\n",
      "349\n",
      "1002\n",
      "1150\n",
      "1577\n",
      "2336\n",
      "2442\n",
      "2587\n",
      "2670\n",
      "1169\n",
      "posting86 ( P & L ) - Hold others accountable\n",
      "1438\n",
      "1711\n",
      "1817\n",
      "1888\n",
      "posting88 08401 \" SUMMARY The\n",
      "posting88 the Store Manager and / or Assistant Store Manager\n",
      "posting88 the Store Manager and / or Assistant Store Manager\n",
      "633\n",
      "1096\n",
      "posting88 the Store Manager and / or Assistant Store Manager\n",
      "posting88 1 ) year\n",
      "posting88 2 ) year\n",
      "posting88 Equal Opportunity Employer Minorities / Women / Protected Veterans /\n",
      "posting89 QUALIFICATIONS ( Education , Experience ,\n",
      "posting89 Certifications ) - Requires\n",
      "3168\n",
      "posting89 EEO / Affirmative Action Employer -- Minorities / Women / Protected Veterans / Disabled\n",
      "273\n",
      "3016\n",
      "posting91 the Los Angeles Fair Chance Initiative for Hiring ( Ban the Box\n",
      "439\n",
      "615\n",
      "894\n",
      "1351\n",
      "posting92 High School Diploma / GED\n",
      "3654\n",
      "3674\n",
      "1120\n",
      "788\n",
      "1230\n",
      "1157\n",
      "1522\n",
      "1561\n",
      "posting94 CRM / DSM\n",
      "564\n",
      "679\n"
     ]
    }
   ],
   "source": [
    "# CREATING POSTING TEXT FILES AND RESPECTIVE ENTITY ANNOTATION FILES FOR BRAT TEXT ANNOTATION TOOL (COGCOMP)\n",
    "# TRIED BOTH CONLL NER AND ONTONOTES NER (doc.get_ner_conll), ONTONOTES SEEMED TO DETECT MORE\n",
    "\n",
    "from ccg_nlpy import remote_pipeline\n",
    "\n",
    "pipeline = remote_pipeline.RemotePipeline()\n",
    "\n",
    "def find_substring(substring, string):\n",
    "    \"\"\" \n",
    "    Returns list of indices where substring begins in string\n",
    "\n",
    "    >>> find_substring('me', \"The cat says meow, meow\")\n",
    "    [13, 19]\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    index = -1  # Begin at -1 so index + 1 is 0\n",
    "    while True:\n",
    "        # Find next index of substring, by starting search from index + 1\n",
    "        index = string.find(substring, index + 1)\n",
    "        if index == -1:  \n",
    "            break  # All occurrences have been found\n",
    "        indices.append(index)\n",
    "    return indices\n",
    "\n",
    "posting_index = 1\n",
    "for r in results:\n",
    "    posting_fname = 'posting' + str(posting_index)\n",
    "    \n",
    "    entity_index = 1\n",
    "    entity_ann_file_text = ''\n",
    "\n",
    "    reg = re.compile(r\"^\\s+\", re.MULTILINE)\n",
    "    test = reg.sub(\" \", re.sub(r'  ', ' ', r['JD_SCRUBBED']))\n",
    "    test = re.sub(\"\\n\", \"\", test)\n",
    "    test = re.sub('(?<=\\w)([:+\\',/\\%])', r' \\1', test)\n",
    "    document = pipeline.doc(test)\n",
    "\n",
    "    posting_text = document.get_text\n",
    "    \n",
    "    with open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/NER/CogCompNLPy/' + posting_fname + '.txt', 'w') as text_file:\n",
    "        text_file.write(posting_text)\n",
    "        text_file.close()\n",
    "    \n",
    "    entities = dict()\n",
    "    if (str(document.get_ner_ontonotes) != 'NER_ONTONOTES view: this view does not have constituents in your input text. '):\n",
    "        for entity in document.get_ner_ontonotes:\n",
    "            mentions = find_substring(entity['tokens'], posting_text)\n",
    "#             if mentions == []:\n",
    "#                 print(posting_fname, entity['tokens'])\n",
    "        \n",
    "            elif entity['tokens'] in entities.keys():\n",
    "                if entities[entity['tokens']] < len(mentions):\n",
    "                    start = mentions[entities[entity['tokens']]]\n",
    "                    entities[entity['tokens']] += 1\n",
    "                    print(start)\n",
    "                    entity_ann_file_text += 'T' + str(entity_index) + '\\t' + spacy_entity_types[entity['label']] + ' ' \\\n",
    "                                                + str(start) + ' ' + str(start+len(entity['tokens'])) + '\\t' \\\n",
    "                                                + entity['tokens'] + '\\n'\n",
    "    \n",
    "                    entity_index += 1\n",
    "            else:\n",
    "                start = mentions[0]\n",
    "                entities[entity['tokens']] = 1\n",
    "                entity_ann_file_text += 'T' + str(entity_index) + '\\t' + spacy_entity_types[entity['label']] + ' ' \\\n",
    "                                                + str(start) + ' ' + str(start+len(entity['tokens'])) + '\\t' \\\n",
    "                                                + entity['tokens'] + '\\n'\n",
    "                entity_index += 1\n",
    "    \n",
    "        ann_file = open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/NER/CogCompNLPy/' + posting_fname + '.ann', 'w')\n",
    "        ann_file.write(entity_ann_file_text)\n",
    "        ann_file.close()\n",
    "    \n",
    "        posting_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'ORG', 'score': 1.0, 'start': 1, 'end': 4, 'tokens': 'DC Area Manager'}\n",
      "[4]\n",
      "DC Area Manager\n",
      "{'label': 'ORG', 'score': 1.0, 'start': 12, 'end': 15, 'tokens': 'the Distribution Center'}\n",
      "[78]\n",
      "the Distribution Center\n",
      "{'label': 'ORG', 'score': 1.0, 'start': 116, 'end': 117, 'tokens': 'OSHA'}\n",
      "[785]\n",
      "OSHA\n",
      "{'label': 'GPE', 'score': 1.0, 'start': 268, 'end': 270, 'tokens': 'Operations Management'}\n",
      "[1801]\n",
      "Operations Management\n",
      "{'label': 'ORG', 'score': 1.0, 'start': 271, 'end': 273, 'tokens': 'Business Administration'}\n",
      "[1824]\n",
      "Business Administration\n",
      "{'label': 'ORG', 'score': 1.0, 'start': 277, 'end': 280, 'tokens': 'Transportation - Working'}\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-09befbc9bacb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposting_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "posting_text = results[0]['JD_SCRUBBED']\n",
    "# reg = re.compile(r\"^\\s+\", re.MULTILINE)\n",
    "# posting_text = reg.sub(\"\", re.sub(r'  ', ' ', results[0]['JD_SCRUBBED']))\n",
    "# posting_text = re.sub(\"\\n\", \"\", posting_text)\n",
    "doc1 = pipeline.doc(posting_text)\n",
    "# print(posting_text)\n",
    "\n",
    "for ent in doc1.get_ner_ontonotes:\n",
    "        entity_ann_file_text += 'T' + str(entity_index) + '\\t' + spacy_entity_types[ent['label']] + ' ' \\\n",
    "                                    + str(ent['start']) + ' ' + str(ent['end']) + '\\t' \\\n",
    "                                    + ent['tokens'] + '\\n'\n",
    "        start = find_substring(ent['tokens'], results[0]['JD_SCRUBBED'])\n",
    "        print(ent)\n",
    "        print(start)\n",
    "        print(posting_text[start[0]:(start[0]+len(ent['tokens']))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "reg = re.compile(r\"^\\s+\", re.MULTILINE)\n",
    "test = reg.sub(\" \", re.sub(r'  ', ' ', results[31]['JD_SCRUBBED']))\n",
    "test = re.sub(\"\\n\", \"\", test)\n",
    "test = re.sub('(?<=\\w)([:+\\',)(])', r' \\1', test)\n",
    "document = pipeline.doc(test)\n",
    "\n",
    "posting_text = document.get_text\n",
    "\n",
    "entity_ann_file_text = ''\n",
    "s = str(document.get_ner_ontonotes)\n",
    "print(s == 'NER_ONTONOTES view: this view does not have constituents in your input text. ')\n",
    "\n",
    "# print(type(document.get_ner_ontonotes))\n",
    "\n",
    "# for ent in document.get_ner_ontonotes:\n",
    "#         start = find_substring(ent['tokens'], posting_text)\n",
    "#         print(ent)\n",
    "#         print(start)\n",
    "#         entity_ann_file_text += 'T' + str(entity_index) + '\\t' + spacy_entity_types[ent['label']] + ' ' \\\n",
    "#                                     + str(start[0]) + ' ' + str(start[0]+len(ent['tokens'])) + '\\t' \\\n",
    "#                                     + ent['tokens'] + '\\n'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING POSTING TEXT FILES AND RESPECTIVE ENTITY ANNOTATION FILES FOR BRAT TEXT ANNOTATION TOOL (GCP)\n",
    "# SET ENVIRONMENT VARIABLE 'GOOGLE_APPLICATION_CREDENTIALS=/Users/nazih.bissat/Downloads/My_First_Project-12919f080214.json'\n",
    "# BEFORE RUNNING JUPYTER NOTEBOOK\n",
    "from google.cloud import language\n",
    "import pickle\n",
    "\n",
    "client = language.LanguageServiceClient()\n",
    "\n",
    "# Load results pickle file\n",
    "pkl_file = open('postings-6-29.pkl', 'rb')\n",
    "results = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# entity types from enums.Entity.Type\n",
    "gcp_entity_types = ['UNKNOWN', 'PERSON', 'LOCATION', 'ORGANIZATION','EVENT', 'WORK_OF_ART', 'CONSUMER_GOOD','OTHER']\n",
    "gcp_entity_dict = {'UNKNOWN': 'Unknown', 'PERSON': 'Person', 'LOCATION': 'Location', 'ORGANIZATION': 'Organization',\n",
    "                  'EVENT': 'Event', 'WORK_OF_ART': 'Work_of_Art', 'CONSUMER_GOOD': 'Consumer_Good', 'OTHER': 'Other'}\n",
    "\n",
    "posting_index = 1\n",
    "for r in results:\n",
    "    posting_fname = 'posting' + str(posting_index)\n",
    "    posting_text = r['JD_SCRUBBED']\n",
    "    document = language.types.Document(\n",
    "        content=posting_text,\n",
    "        type=language.enums.Document.Type.PLAIN_TEXT,\n",
    "    )\n",
    "    \n",
    "    with open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/NER/GCP/' + posting_fname + '.txt', 'w') as text_file:\n",
    "        text_file.write(posting_text)\n",
    "        text_file.close()\n",
    "    \n",
    "    response = client.analyze_entities(\n",
    "        document=document,\n",
    "        encoding_type='UTF32',\n",
    "    )\n",
    "    \n",
    "    entity_index = 1\n",
    "    entity_ann_file_text = ''\n",
    "    dups = dict()\n",
    "    \n",
    "    for entity in response.entities:\n",
    "        for mention in entity.mentions:\n",
    "            start = mention.text.begin_offset\n",
    "            entity_ann_file_text += 'T' + str(entity_index) + '\\t' + gcp_entity_dict[gcp_entity_types[entity.type]] + ' ' \\\n",
    "                                        + str(start) + ' ' + str(start + len(entity.name)) + '\\t' \\\n",
    "                                        + entity.name + '\\n'\n",
    "            entity_index += 1\n",
    "    \n",
    "    ann_file = open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/NER/GCP/' + posting_fname + '.ann', 'w')\n",
    "    ann_file.write(entity_ann_file_text)\n",
    "    ann_file.close()\n",
    "    \n",
    "    posting_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description\n",
      "\n",
      "Req/Job ID: 934314BR  \n",
      "Employing Entity: Sears, Roebuck and Co.  \n",
      "Employment Category: Regular, Part-time  \n",
      "Job Function: Automotive  \n",
      "Store ID: 06371: Sears Auto Center  \n",
      "\n",
      "If you are looking to become part of an auto center that really values your skills and ability to provide quality service, consider joining the sears auto center team . Now is the perfect time to join as we are going through an exciting transformation of our business!! The Express Service Technician is responsible for the successful tire, battery, and oil installation to customer vehicles following all automotive processes and procedures. This position interacts daily with customers, Customer Service Adviser, other technicians, and the Auto Center Manager.  \n",
      "\n",
      "[#video#https://www.youtube.com/watch?v=YhZoUk2cM3Y{#560,315#}#/video#]\n",
      "\n",
      "Job Requirements\n",
      "\n",
      "Job Duties/Responsibilities:  \n",
      "• Participates actively with team servicing of every customer vehicle to ensure fast, expert service  \n",
      "• Dedicates work time to providing excellent customer service  \n",
      "• Promotes teamwork to deliver times and accurate customer care during all operating hours  \n",
      "• Demonstrates a sense of responsive urgency to every customer no matter what service is required  \n",
      "• Uses the Quality Service Evaluation (QSE) form to understand customer service adviser's and customer service manager's instructions  \n",
      "• Fulfills the customer's needs the first time, every time  \n",
      "• Communicates with Auto Center Manager, Customer Service Adviser and other Technicians to meet Sears Automotive time standards and to exceed customer requirements.  \n",
      "• Adheres strictly to Sears Automotive Dress Code standards  \n",
      "• Protects customer's vehicle with floor mats, seat/steering wheel covers and, if necessary, fender covers  \n",
      "• Racks vehicle safely following Sears Automotive training  \n",
      "• Performs Multi Point Inspection (MPI) visual inspections and communicate with customer service manager and/or customer service adviser within 10 minutes  \n",
      "• Maintains back shop cleanliness and equipment calibration to Sears Automotive standards  \n",
      "• Thanks customer, expressing appreciation for the business  \n",
      "• Performs miscellaneous duties as assigned  \n",
      "\n",
      "Required Skills:  \n",
      "• Valid Drivers License  \n",
      "• Successful completion of Sears Automotive Express Technician core curriculum  \n",
      "• Commitment to safety: Use of appropriate personal protective equipment, back belt, safety glasses and safety shoes at all times  \n",
      "• Maintenance of individual productivity as defined by the business sales per hour standard  \n",
      "• Ability to stand and walk for prolonged periods of time  \n",
      "• Ability to lift up to 50 pounds  \n",
      "• Extended workdays of up to 10 hours, weekends and nights as necessary  \n",
      "• Ability to handle stressful situations and work in a fast-paced environment  \n",
      "• Ability to read and utilize reports  \n",
      "• Extensive spoken communication for customer and associate relationship skills  \n",
      "• Must have a valid driver's license  \n",
      "\n",
      "Preferred Skills:  \n",
      "• Speed and expertise when installing tires and batteries  \n",
      "• Commitment to teamwork and mentoring others  \n",
      "\n",
      "Education Requirements: HS Graduate or Equivalent  \n",
      "License/Certificate Required: Yes  \n",
      "Driver's License Required: Yes  \n",
      "Age Requirement: 18+  \n",
      "\n",
      "#Stores, #Automotive\n",
      "name: \"Sears Automotive\"\n",
      "type: ORGANIZATION\n",
      "salience: 0.008043723180890083\n",
      "mentions {\n",
      "  text {\n",
      "    content: \"Sears Automotive\"\n",
      "    begin_offset: 1531\n",
      "  }\n",
      "  type: PROPER\n",
      "}\n",
      "mentions {\n",
      "  text {\n",
      "    content: \"Sears Automotive\"\n",
      "    begin_offset: 2054\n",
      "  }\n",
      "  type: PROPER\n",
      "}\n",
      "\n",
      "mentions: 1531\n"
     ]
    }
   ],
   "source": [
    "# EDA (GCP)\n",
    "from google.cloud import language\n",
    "\n",
    "\n",
    "client = language.LanguageServiceClient()\n",
    "\n",
    "posting_text = results[0]['JD_SCRUBBED']\n",
    "document = language.types.Document(\n",
    "        content=posting_text,\n",
    "        type=language.enums.Document.Type.PLAIN_TEXT,\n",
    "    )\n",
    "\n",
    "response = client.analyze_entities(\n",
    "        document=document,\n",
    "        encoding_type='UTF32',\n",
    "    )\n",
    "\n",
    "print(posting_text)\n",
    "\n",
    "for entity in response.entities:\n",
    "    if len(entity.mentions) > 1:\n",
    "        print(entity)\n",
    "        print('mentions:', entity.mentions[0].text.begin_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING POSTING TEXT FILES AND RESPECTIVE ENTITY ANNOTATION FILES FOR BRAT TEXT ANNOTATION TOOL (APACHE OPENNLP)\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "apache_entity_types = {'person':'Person', 'location':'Location', 'date':'Date', 'time':'Time', 'money':'Money',\n",
    "                      'organization': 'Organization', 'percentage':'Percent'}\n",
    "\n",
    "def find_substring(substring, string):\n",
    "    \"\"\" \n",
    "    Returns list of indices where substring begins in string\n",
    "\n",
    "    >>> find_substring('me', \"The cat says meow, meow\")\n",
    "    [13, 19]\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    index = -1  # Begin at -1 so index + 1 is 0\n",
    "    while True:\n",
    "        # Find next index of substring, by starting search from index + 1\n",
    "        index = string.find(substring, index + 1)\n",
    "        if index == -1:  \n",
    "            break  # All occurrences have been found\n",
    "        indices.append(index)\n",
    "    return indices\n",
    "\n",
    "# THIS IS THE COMMAND I RAN FOR IT TO WORK:\n",
    "# opennlp TokenNameFinder en-ner-person.bin en-ner-location.bin \n",
    "# en-ner-time.bin en-ner-money.bin ener-organization.bin en-ner-percentage.bin en-ner-date.bin\n",
    "\n",
    "batch_posting = ''\n",
    "for r in results:\n",
    "    batch_posting += r['JD_SCRUBBED'] + '\\n\\n\\n'\n",
    "    \n",
    "with open('//Users/nazih.bissat/Desktop/apache_opennlp/apache-opennlp-1.8.4/bin/batch_postings.txt', 'w') as text_file:\n",
    "    text_file.write(batch_posting)\n",
    "    text_file.close()\n",
    "\n",
    "commands = '''\n",
    "cd /Users/nazih.bissat/Desktop/apache_opennlp/apache-opennlp-1.8.4/bin\n",
    "cat batch_postings.txt | opennlp TokenNameFinder en-ner-person.bin en-ner-location.bin en-ner-time.bin en-ner-money.bin en-ner-organization.bin en-ner-percentage.bin en-ner-date.bin\n",
    "'''\n",
    "\n",
    "process = Popen('/bin/bash', stdin=PIPE, stdout=PIPE)\n",
    "out, err = process.communicate(commands.encode('utf-8'))\n",
    "\n",
    "full_entities = out.decode('utf-8')\n",
    "\n",
    "posting_index = 1\n",
    "for posting in full_entities.split('\\n\\n\\n'):\n",
    "    posting_fname = 'posting' + str(posting_index)\n",
    "    \n",
    "    posting_text = re.sub(r'\\<START:[a-z]*\\> ', '', posting)\n",
    "    posting_text = re.sub(r' \\<END\\>', '', posting_text)\n",
    "    \n",
    "    with open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/NER/Apache/' + posting_fname + '.txt', 'w') as text_file:\n",
    "        text_file.write(posting_text)\n",
    "        text_file.close()\n",
    "    \n",
    "    entity_ann_file_text = ''\n",
    "    counter = 0\n",
    "    entity_index = 1\n",
    "    for start in find_substring('<START:', posting):\n",
    "        end_type = find_substring('>', posting)[2 * (entity_index - 1)]\n",
    "        e_type = posting[(start+7):(end_type)]\n",
    "        end_name = find_substring('<END>', posting)[entity_index - 1]\n",
    "        name = posting[(end_type+2):(end_name-1)]\n",
    "        entity_ann_file_text += 'T' + str(entity_index) + '\\t' + apache_entity_types[e_type] + ' ' \\\n",
    "                                    + str(start-counter) + ' ' + str(start-counter+len(name)) + '\\t' \\\n",
    "                                    + name + '\\n'\n",
    "        counter += 15 + len(e_type)\n",
    "        entity_index += 1\n",
    "    \n",
    "    ann_file = open('/Users/nazih.bissat/Desktop/brat-v1.3_Crunchy_Frog/data/NER/Apache/' + posting_fname + '.ann', 'w')\n",
    "    ann_file.write(entity_ann_file_text)\n",
    "    ann_file.close()\n",
    "    \n",
    "    posting_index += 1\n",
    "    \n",
    "commands = '''\n",
    "rm /Users/nazih.bissat/Desktop/apache_opennlp/apache-opennlp-1.8.4/bin/batch_postings.txt\n",
    "'''\n",
    "process = Popen('/bin/bash', stdin=PIPE, stdout=PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description\n",
      "\n",
      "Req/Job ID: 934314BR\n",
      "Employing Entity: Sears, Roebuck and Co.\n",
      "Employment Category: Regular, Part-time\n",
      "Job Function: Automotive\n",
      "Store ID: 06371: Sears Auto Center\n",
      "\n",
      "If you are looking to become part of an auto center that really values your skills and ability to provide quality service, consider joining the sears auto center team . Now is the perfect time to join as we are going through an exciting transformation of our business!! The Express Service Technician is responsible for the successful tire, battery, and oil installation to customer vehicles following all automotive processes and procedures. This position interacts daily with customers, Customer Service Adviser, other technicians, and the Auto Center Manager.\n",
      "\n",
      "[#video#https://www.youtube.com/watch?v=YhZoUk2cM3Y{#560,315#}#/video#] \n",
      "\n",
      "Job Requirements\n",
      "\n",
      "Job Duties/Responsibilities:\n",
      "• Participates actively with team servicing of every customer vehicle to ensure fast, expert service\n",
      "• Dedicates work time to providing excellent customer service\n",
      "• Promotes teamwork to deliver times and accurate customer care during all operating hours\n",
      "• Demonstrates a sense of responsive urgency to every customer no matter what service is required\n",
      "• Uses the Quality Service Evaluation (QSE) form to understand customer service adviser's and customer service manager's instructions\n",
      "• Fulfills the customer's needs the first time, every time\n",
      "• Communicates with Auto Center Manager, Customer Service Adviser and other Technicians to meet Sears Automotive time standards and to exceed customer requirements.\n",
      "• Adheres strictly to Sears Automotive Dress Code standards\n",
      "• Protects customer's vehicle with floor mats, seat/steering wheel covers and, if necessary, fender covers\n",
      "• Racks vehicle safely following Sears Automotive training\n",
      "• Performs Multi Point Inspection (MPI) visual inspections and communicate with customer service manager and/or customer service adviser within 10 minutes\n",
      "• Maintains back shop cleanliness and equipment calibration to Sears Automotive standards\n",
      "• Thanks customer, expressing appreciation for the business\n",
      "• Performs miscellaneous duties as assigned\n",
      "\n",
      "Required Skills:\n",
      "• Valid Drivers License\n",
      "• Successful completion of Sears Automotive Express Technician core curriculum\n",
      "• Commitment to safety: Use of appropriate personal protective equipment, back belt, safety glasses and safety shoes at all times\n",
      "• Maintenance of individual productivity as defined by the business sales per hour standard\n",
      "• Ability to stand and walk for prolonged periods of time\n",
      "• Ability to lift up to 50 pounds\n",
      "• Extended workdays of up to 10 hours, weekends and nights as necessary\n",
      "• Ability to handle stressful situations and work in a fast-paced environment\n",
      "• Ability to read and utilize reports\n",
      "• Extensive spoken communication for customer and associate relationship skills\n",
      "• Must have a valid driver's license\n",
      "\n",
      "Preferred Skills:\n",
      "• Speed and expertise when installing tires and batteries\n",
      "• Commitment to teamwork and mentoring others\n",
      "\n",
      "Education Requirements: HS Graduate or Equivalent\n",
      "License/Certificate Required: Yes\n",
      "Driver's License Required: Yes\n",
      "Age Requirement: 18+\n",
      "\n",
      "#Stores, #Automotive\n",
      "\n",
      "\n",
      "\n",
      "Job Description\n",
      "\n",
      "Req/Job ID: 934314BR\n",
      "Employing <START:person> Entity: Sears, Roebuck <END> and Co.\n",
      "Employment Category: Regular, Part-time\n",
      "Job <START:organization> Function: Automotive <END>\n",
      "<START:organization> Store <END> <START:organization> ID: 06371: Sears Auto Center <END>\n",
      "\n",
      "If you are looking to become part of an auto center that really values your skills and ability to provide quality service, consider joining the sears auto center team . Now is the perfect time to join as we are going through an exciting transformation of our business!! <START:organization> The Express Service Technician <END> is responsible for the successful tire, battery, and oil installation to customer vehicles following all automotive processes and procedures. This position interacts daily with customers, <START:organization> Customer Service Adviser, <END> other technicians, and the <START:organization> Auto Center Manager. <END>\n",
      "\n",
      "[#video#https://www.youtube.com/watch?v=YhZoUk2cM3Y{#560,315#}#/video#] \n",
      "\n",
      "Job Requirements\n",
      "\n",
      "Job Duties/Responsibilities:\n",
      "• Participates actively with team servicing of every customer vehicle to ensure fast, expert service\n",
      "• Dedicates work time to providing excellent customer service\n",
      "• Promotes teamwork to deliver times and accurate customer care during all operating hours\n",
      "• Demonstrates a sense of responsive urgency to every customer no matter what service is required\n",
      "• Uses the <START:organization> Quality Service Evaluation <END> (QSE) form to understand customer service adviser's and customer service manager's instructions\n",
      "• Fulfills the customer's needs the first time, every time\n",
      "• Communicates with <START:organization> Auto Center Manager, Customer Service <END> Adviser and other Technicians to meet <START:organization> Sears Automotive <END> time standards and to exceed customer requirements.\n",
      "• Adheres strictly to <START:organization> Sears Automotive Dress Code <END> standards\n",
      "• Protects customer's vehicle with floor mats, seat/steering wheel covers and, if necessary, fender covers\n",
      "• Racks vehicle safely following <START:organization> Sears Automotive <END> training\n",
      "• Performs <START:organization> Multi Point Inspection <END> (MPI) visual inspections and communicate with customer service manager and/or customer service adviser within 10 minutes\n",
      "• Maintains back shop cleanliness and equipment calibration to <START:organization> Sears Automotive <END> standards\n",
      "• Thanks customer, expressing appreciation for the business\n",
      "• Performs miscellaneous duties as assigned\n",
      "\n",
      "Required Skills:\n",
      "• Valid Drivers License\n",
      "• Successful completion of <START:organization> Sears Automotive Express Technician <END> core curriculum\n",
      "• Commitment to safety: Use of appropriate personal protective equipment, back belt, safety glasses and safety shoes at all times\n",
      "• Maintenance of individual productivity as defined by the business sales per hour standard\n",
      "• Ability to stand and walk for prolonged periods of time\n",
      "• Ability to lift up to 50 pounds\n",
      "• Extended workdays of up to 10 hours, weekends and nights as necessary\n",
      "• Ability to handle stressful situations and work in a fast-paced environment\n",
      "• Ability to read and utilize reports\n",
      "• Extensive spoken communication for customer and associate relationship skills\n",
      "• Must have a valid driver's license\n",
      "\n",
      "Preferred Skills:\n",
      "• Speed and expertise when installing tires and batteries\n",
      "• Commitment to teamwork and mentoring others\n",
      "\n",
      "<START:organization> Education Requirements: HS Graduate <END> or Equivalent\n",
      "License/Certificate Required: Yes\n",
      "Driver's License Required: Yes\n",
      "Age Requirement: 18+\n",
      "\n",
      "#Stores, #Automotive\n",
      "\n",
      "\n",
      "\n",
      "name: Entity: Sears, Roebuck\n",
      "type: person\n",
      "start: 48\n",
      "end: 70\n",
      "check name in original sentence: Entity: Sears, Roebuck\n",
      "\n",
      "\n",
      "name: Function: Automotive\n",
      "type: organization\n",
      "start: 123\n",
      "end: 143\n",
      "check name in original sentence: Function: Automotive\n",
      "\n",
      "\n",
      "name: Store\n",
      "type: organization\n",
      "start: 144\n",
      "end: 149\n",
      "check name in original sentence: Store\n",
      "\n",
      "\n",
      "name: ID: 06371: Sears Auto Center\n",
      "type: organization\n",
      "start: 150\n",
      "end: 178\n",
      "check name in original sentence: ID: 06371: Sears Auto Center\n",
      "\n",
      "\n",
      "name: The Express Service Technician\n",
      "type: organization\n",
      "start: 450\n",
      "end: 480\n",
      "check name in original sentence: The Express Service Technician\n",
      "\n",
      "\n",
      "name: Customer Service Adviser,\n",
      "type: organization\n",
      "start: 669\n",
      "end: 694\n",
      "check name in original sentence: Customer Service Adviser,\n",
      "\n",
      "\n",
      "name: Auto Center Manager.\n",
      "type: organization\n",
      "start: 722\n",
      "end: 742\n",
      "check name in original sentence: Auto Center Manager.\n",
      "\n",
      "\n",
      "name: Quality Service Evaluation\n",
      "type: organization\n",
      "start: 1228\n",
      "end: 1254\n",
      "check name in original sentence: Quality Service Evaluation\n",
      "\n",
      "\n",
      "name: Auto Center Manager, Customer Service\n",
      "type: organization\n",
      "start: 1430\n",
      "end: 1467\n",
      "check name in original sentence: Auto Center Manager, Customer Service\n",
      "\n",
      "\n",
      "name: Sears Automotive\n",
      "type: organization\n",
      "start: 1506\n",
      "end: 1522\n",
      "check name in original sentence: Sears Automotive\n",
      "\n",
      "\n",
      "name: Sears Automotive Dress Code\n",
      "type: organization\n",
      "start: 1597\n",
      "end: 1624\n",
      "check name in original sentence: Sears Automotive Dress Code\n",
      "\n",
      "\n",
      "name: Sears Automotive\n",
      "type: organization\n",
      "start: 1775\n",
      "end: 1791\n",
      "check name in original sentence: Sears Automotive\n",
      "\n",
      "\n",
      "name: Multi Point Inspection\n",
      "type: organization\n",
      "start: 1812\n",
      "end: 1834\n",
      "check name in original sentence: Multi Point Inspection\n",
      "\n",
      "\n",
      "name: Sears Automotive\n",
      "type: organization\n",
      "start: 2019\n",
      "end: 2035\n",
      "check name in original sentence: Sears Automotive\n",
      "\n",
      "\n",
      "name: Sears Automotive Express Technician\n",
      "type: organization\n",
      "start: 2219\n",
      "end: 2254\n",
      "check name in original sentence: Sears Automotive Express Technician\n",
      "\n",
      "\n",
      "name: Education Requirements: HS Graduate\n",
      "type: organization\n",
      "start: 3014\n",
      "end: 3049\n",
      "check name in original sentence: Education Requirements: HS Graduate\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EDA FOR ABOVE CODE, TRYING TO FIND INSTANCES WHERE BUG IS CAUSED\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "def find_substring(substring, string):\n",
    "    \"\"\" \n",
    "    Returns list of indices where substring begins in string\n",
    "\n",
    "    >>> find_substring('me', \"The cat says meow, meow\")\n",
    "    [13, 19]\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    index = -1  # Begin at -1 so index + 1 is 0\n",
    "    while True:\n",
    "        # Find next index of substring, by starting search from index + 1\n",
    "        index = string.find(substring, index + 1)\n",
    "        if index == -1:  \n",
    "            break  # All occurrences have been found\n",
    "        indices.append(index)\n",
    "    return indices\n",
    "\n",
    "commands = '''\n",
    "cd /Users/nazih.bissat/Desktop/apache_opennlp/apache-opennlp-1.8.4/bin\n",
    "cat test.txt | opennlp TokenNameFinder en-ner-person.bin en-ner-location.bin en-ner-time.bin en-ner-money.bin en-ner-organization.bin en-ner-percentage.bin en-ner-date.bin\n",
    "'''\n",
    "\n",
    "process = Popen('/bin/bash', stdin=PIPE, stdout=PIPE)\n",
    "out, err = process.communicate(commands.encode('utf-8'))\n",
    "\n",
    "ents = out.decode('utf-8')\n",
    "sent = re.sub(r'\\<START:[a-z]*\\> ', '', ents)\n",
    "sent = re.sub(r' \\<END\\>', '', sent)\n",
    "# reg = re.compile(r\"^\\s+\", re.MULTILINE)\n",
    "# # sent = reg.sub(\"\", results[0]['JD_SCRUBBED'])\n",
    "# sent = re.sub(r'  ', '', results[0]['JD_SCRUBBED'])\n",
    "# test = re.sub(\"\\n\", \"\", test)\n",
    "# sent = re.sub('(?<=\\w)([:+\\',/\\%])', r' \\1', test)\n",
    "\n",
    "print(sent)\n",
    "print('\\n')\n",
    "print(ents)\n",
    "print('\\n')\n",
    "\n",
    "counter = 0\n",
    "entity_index = 1\n",
    "for start in find_substring('<START:', ents):\n",
    "    end_type = find_substring('>', ents)[2 * (entity_index - 1)]\n",
    "    e_type = ents[(start+7):(end_type)]\n",
    "    end_name = find_substring('<END>', ents)[entity_index - 1]\n",
    "    name = ents[(end_type+2):(end_name-1)]\n",
    "    print('name: {0}'.format(name))\n",
    "    print('type: {0}'.format(e_type))\n",
    "    print('start: {0}'.format(start-counter))\n",
    "    print('end: {0}'.format(start-counter+len(name)))\n",
    "    print('check name in original sentence: {0}'.format(sent[(start-counter):(start-counter+len(name))]))\n",
    "#     print('rest of original sentence: {0}'.format(sent[(start-counter+len(name)):]))\n",
    "    print('\\n')\n",
    "    counter += 15 + len(e_type)\n",
    "    entity_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "5\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len('<START:>  <END>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employing Entity: Sears, Roebuck and Co.\n"
     ]
    }
   ],
   "source": [
    "s = 'Employing <START:person> Entity: Sears, Roebuck <END> and Co.'\n",
    "s = re.sub(r'\\<START:[a-z]*\\> ', '', s)\n",
    "s = re.sub(r' \\<END\\>', '', s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employing <START:person> Entity: Sears, Roebuck <END> and Co.\n"
     ]
    }
   ],
   "source": [
    "test = 'Employing <START:person> Entity: Sears, Roebuck <END> and Co.'\n",
    "print(re.sub(r'(\\<START\\:.*?\\> )?(.*?)( \\<END\\>)$', \"\", test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
